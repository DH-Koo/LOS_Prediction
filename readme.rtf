By applying ‘DL_data_preprocessing.ipynb’, ‘los_dataset_24h.parquet’ was made. And this was used for all model training.


I put all experimental source codes(model). Each model’s brief description is following:
	1st model: baseline model, concat all features and apply MLP (MAE: 63.74)
	2nd model: change the predicted target to log-scale LOS (MAE: 59.12)
	3rd model: give more weight to tail-data for sampling (MAE: 59.72)
	4th model: make branches for features and apply MLP respectively (MAE: 57.84)
	5th model(final model): learn attention weight for each branch (MAE: 57.73)
	6th model: use transformer encoder for learning weight of each branch (MAE: 58.48)
	7th model: change architecture to use token-level transformer rather than multi-branch MLP (MAE: 60.83)


For quick review of each model’s performance, you can see the following output cell in each source code.

===== Test set MAE =====
Test MAE (hours): 63.74
Test MAE (days) : 2.66
