{\rtf1\ansi\ansicpg949\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww12960\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \
\
By applying \'91DL_data_preprocessing.ipynb\'92, \'91los_dataset_24h.parquet\'92 was made. And this was used for all model training.\
\
\
I put all experimental source codes(model). Each model\'92s brief description is following:\
	1st model: baseline model, concat all features and apply MLP (MAE: 63.74)\
	2nd model: change the predicted target to log-scale LOS (MAE: 59.12)\
	3rd model: give more weight to tail-data for sampling (MAE: 59.72)\
	4th model: make branches for features and apply MLP respectively (MAE: 57.84)\
	5th model(final model): learn attention weight for each branch (MAE: 57.73)\
	6th model: use transformer encoder for learning weight of each branch (MAE: 58.48)\
	7th model: change architecture to use token-level transformer rather than multi-branch MLP (MAE: 60.83)\
\
\
For quick review of each model\'92s performance, you can see the following output cell in each source code.\
\pard\pardeftab720\partightenfactor0

\f1 \cf2 \expnd0\expndtw0\kerning0
\
===== Test set MAE =====\
Test MAE (hours): 63.74\
Test MAE (days) : 2.66}