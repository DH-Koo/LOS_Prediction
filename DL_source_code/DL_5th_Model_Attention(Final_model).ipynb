{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V08e05Ftqyv",
        "outputId": "82dc64ab-2993-491d-d77e-3f42f6f1f96c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(424803, 17)\n",
            "Index(['subject_id', 'hadm_id', 'admittime', 'dischtime', 'race', 'los_hours',\n",
            "       'gender', 'anchor_age', 'curr_service', 'hcpcs_cd_list',\n",
            "       'diagnoses_icd_code_list', 'procedures_icd_code_list', 'drg_code',\n",
            "       'drg_severity', 'drg_mortality', 'medication_list', 'order_type_list'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_parquet(\"los_dataset_24h.parquet\")\n",
        "\n",
        "print(df.shape)\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTrTAv2ht3aZ"
      },
      "outputs": [],
      "source": [
        "def build_label_encoder(series):\n",
        "    classes = sorted(series.unique())\n",
        "    stoi = {c: i for i, c in enumerate(classes)}\n",
        "    return stoi\n",
        "\n",
        "def build_vocab_from_list_column(df, col, min_freq=1, add_unk=True):\n",
        "    counter = Counter()\n",
        "    for lst in df[col]:\n",
        "        counter.update(lst)\n",
        "\n",
        "    stoi = {}\n",
        "    idx = 0\n",
        "    if add_unk:\n",
        "        stoi[\"<UNK>\"] = idx\n",
        "        idx += 1\n",
        "\n",
        "    for token, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            if token not in stoi:\n",
        "                stoi[token] = idx\n",
        "                idx += 1\n",
        "\n",
        "    return stoi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2LpjEFBuIUx",
        "outputId": "fb4f8f5a-1623-4fca-a3b5-bdd3d9affcdc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_genders: 2\n",
            "num_races: 33\n",
            "num_services: 21\n",
            "num_drg_codes: 301\n"
          ]
        }
      ],
      "source": [
        "# Encoder for gender, race, curr_service, drg_code\n",
        "gender_stoi = build_label_encoder(df[\"gender\"])\n",
        "race_stoi = build_label_encoder(df[\"race\"])\n",
        "service_stoi = build_label_encoder(df[\"curr_service\"])\n",
        "\n",
        "# drg_code is already an int, but treated as a \"category\" and re-mapped to an index\n",
        "drg_code_stoi = build_label_encoder(df[\"drg_code\"].astype(int))\n",
        "\n",
        "print(\"num_genders:\", len(gender_stoi))\n",
        "print(\"num_races:\", len(race_stoi))\n",
        "print(\"num_services:\", len(service_stoi))\n",
        "print(\"num_drg_codes:\", len(drg_code_stoi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jysvDQ6KuKI3",
        "outputId": "e0150f07-ecbf-4f9b-e3c6-991298915685"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diag vocab size: 27701\n",
            "proc vocab size: 12184\n",
            "hcpcs vocab size: 1925\n",
            "med vocab size: 3358\n",
            "order vocab size: 17\n"
          ]
        }
      ],
      "source": [
        "list_cols = [\n",
        "    \"diagnoses_icd_code_list\",\n",
        "    \"procedures_icd_code_list\",\n",
        "    \"hcpcs_cd_list\",\n",
        "    \"medication_list\",\n",
        "    \"order_type_list\",\n",
        "]\n",
        "\n",
        "diag_stoi = build_vocab_from_list_column(df, \"diagnoses_icd_code_list\", min_freq=1)\n",
        "proc_stoi = build_vocab_from_list_column(df, \"procedures_icd_code_list\", min_freq=1)\n",
        "hcpcs_stoi = build_vocab_from_list_column(df, \"hcpcs_cd_list\", min_freq=1)\n",
        "med_stoi = build_vocab_from_list_column(df, \"medication_list\", min_freq=1)\n",
        "order_stoi = build_vocab_from_list_column(df, \"order_type_list\", min_freq=1)\n",
        "\n",
        "print(\"diag vocab size:\", len(diag_stoi))\n",
        "print(\"proc vocab size:\", len(proc_stoi))\n",
        "print(\"hcpcs vocab size:\", len(hcpcs_stoi))\n",
        "print(\"med vocab size:\", len(med_stoi))\n",
        "print(\"order vocab size:\", len(order_stoi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prFr0KUjuNiy",
        "outputId": "7f73452a-ea4c-4e51-aafa-3f4dc7e5384b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "combined proc vocab size: 14108\n"
          ]
        }
      ],
      "source": [
        "# Combine proc_stoi and hcpcs_stoi into a single unified vocab\n",
        "proc_all_stoi = {}\n",
        "idx = 0\n",
        "\n",
        "# Only one UNK token\n",
        "proc_all_stoi[\"<UNK>\"] = idx\n",
        "idx += 1\n",
        "\n",
        "# Procedures first\n",
        "for k in proc_stoi.keys():\n",
        "    if k == \"<UNK>\":\n",
        "        continue\n",
        "    proc_all_stoi[\"PROC_\" + k] = idx\n",
        "    idx += 1\n",
        "\n",
        "# HCPCS next\n",
        "for k in hcpcs_stoi.keys():\n",
        "    if k == \"<UNK>\":\n",
        "        continue\n",
        "    key = \"HCPCS_\" + k\n",
        "    if key not in proc_all_stoi:\n",
        "        proc_all_stoi[key] = idx\n",
        "        idx += 1\n",
        "\n",
        "print(\"combined proc vocab size:\", len(proc_all_stoi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Vqz1gkzjuQpZ"
      },
      "outputs": [],
      "source": [
        "UNK_DIAG = diag_stoi[\"<UNK>\"]\n",
        "UNK_PROC = proc_all_stoi[\"<UNK>\"]\n",
        "UNK_MED = med_stoi[\"<UNK>\"]\n",
        "UNK_ORDER = order_stoi[\"<UNK>\"]\n",
        "\n",
        "def map_list_to_ids(lst, stoi, unk_token=\"<UNK>\"):\n",
        "    unk_idx = stoi.get(unk_token, None)\n",
        "    out = []\n",
        "    for x in lst:\n",
        "        idx = stoi.get(x)\n",
        "        if idx is None:\n",
        "            if unk_idx is not None:\n",
        "                out.append(unk_idx)\n",
        "        else:\n",
        "            out.append(idx)\n",
        "    return out\n",
        "\n",
        "# Diagnostic codes\n",
        "df[\"diag_ids\"] = df[\"diagnoses_icd_code_list\"].apply(\n",
        "    lambda lst: map_list_to_ids(lst, diag_stoi)\n",
        ")\n",
        "\n",
        "# Combined procedure + hcpcs IDs\n",
        "def build_proc_ids(row):\n",
        "    ids = []\n",
        "    for code in row[\"procedures_icd_code_list\"]:\n",
        "        tok = \"PROC_\" + code\n",
        "        ids.append(proc_all_stoi.get(tok, UNK_PROC))\n",
        "    for code in row[\"hcpcs_cd_list\"]:\n",
        "        tok = \"HCPCS_\" + code\n",
        "        ids.append(proc_all_stoi.get(tok, UNK_PROC))\n",
        "    return ids\n",
        "\n",
        "df[\"proc_ids\"] = df.apply(build_proc_ids, axis=1)\n",
        "\n",
        "# medication\n",
        "df[\"med_ids\"] = df[\"medication_list\"].apply(\n",
        "    lambda lst: map_list_to_ids(lst, med_stoi)\n",
        ")\n",
        "\n",
        "# order_type\n",
        "df[\"order_ids\"] = df[\"order_type_list\"].apply(\n",
        "    lambda lst: map_list_to_ids(lst, order_stoi)\n",
        ")\n",
        "\n",
        "# Single categorical -> id\n",
        "df[\"gender_id\"] = df[\"gender\"].map(gender_stoi)\n",
        "df[\"race_id\"] = df[\"race\"].map(race_stoi)\n",
        "df[\"service_id\"] = df[\"curr_service\"].map(service_stoi)\n",
        "df[\"drg_code_id\"] = df[\"drg_code\"].astype(int).map(drg_code_stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTxymZbB-_YY",
        "outputId": "b9a6cd21-2837-4c9e-873c-41bba37cfd12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================ SAMPLE DATA (after encoding) ==============\n",
            "\n",
            "--- Row 0 ---\n",
            "gender                    : F\n",
            "race                      : WHITE\n",
            "curr_service              : MED\n",
            "drg_code                  : 279\n",
            "gender_id                 : 0\n",
            "race_id                   : 28\n",
            "service_id                : 7\n",
            "drg_code_id               : 135\n",
            "diagnoses_icd_code_list   : ['07071' '78959' '2875' '2761' '496' '5715' 'V08' '3051']\n",
            "diag_ids                  : [1, 2, 3, 4, 5, 6, 7, 8]\n",
            "procedures_icd_code_list  : ['5491']\n",
            "hcpcs_cd_list             : []\n",
            "proc_ids                  : [1]\n",
            "medication_list           : ['Raltegravir' 'Rifaximin' 'Sodium Chloride 0.9%  Flush'\n",
            " 'Calcium Carbonate' 'Rifaximin' 'Raltegravir'\n",
            " 'Emtricitabine-Tenofovir (Truvada)' 'Sulfameth/Trimethoprim DS'\n",
            " 'Furosemide' 'Tiotropium Bromide' 'Albuterol Inhaler' 'Lactulose'\n",
            " 'Heparin' 'Sodium Chloride 0.9%  Flush' 'Acetaminophen' 'Heparin'\n",
            " 'Lactulose' 'Albumin 25% (12.5g / 50mL)' 'Sodium Chloride 0.9%  Flush'\n",
            " 'Albumin 25% (12.5g / 50mL)' 'Albumin 25% (12.5g / 50mL)']\n",
            "med_ids                   : [1, 2, 3, 4, 2, 1, 5, 6, 7, 8, '...(+11 more)']\n",
            "order_type_list           : ['Medications' 'General Care' 'Nutrition' 'Blood Bank' 'Lab' 'Respiratory'\n",
            " 'Medications' 'Medications' 'ADT orders' 'Lab' 'Lab' 'Medications'\n",
            " 'ADT orders' 'ADT orders' 'ADT orders' 'ADT orders' 'Lab' 'ADT orders'\n",
            " 'Lab' 'General Care' 'Nutrition' 'Medications' 'ADT orders' 'Lab'\n",
            " 'IV therapy' 'Medications' 'General Care' 'General Care' 'Nutrition'\n",
            " 'Medications' 'Nutrition' 'Lab' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'Medications']\n",
            "order_ids                 : [1, 2, 3, 4, 5, 6, 1, 1, 7, 5, '...(+32 more)']\n",
            "\n",
            "\n",
            "--- Row 1 ---\n",
            "gender                    : F\n",
            "race                      : WHITE\n",
            "curr_service              : MED\n",
            "drg_code                  : 283\n",
            "gender_id                 : 0\n",
            "race_id                   : 28\n",
            "service_id                : 7\n",
            "drg_code_id               : 139\n",
            "diagnoses_icd_code_list   : ['07054' '78959' 'V462' '5715' '2767' '2761' '496' 'V08' '3051' '78791']\n",
            "diag_ids                  : [9, 2, 10, 6, 11, 4, 5, 7, 8, 12]\n",
            "procedures_icd_code_list  : ['5491']\n",
            "hcpcs_cd_list             : []\n",
            "proc_ids                  : [1]\n",
            "medication_list           : ['Heparin' 'Raltegravir' 'Rifaximin' 'Emtricitabine-Tenofovir (Truvada)'\n",
            " 'Lactulose' 'Fluticasone Propionate 110mcg' 'Tiotropium Bromide'\n",
            " 'Albuterol Inhaler' 'Calcium Gluconate' 'Dextrose 50%'\n",
            " 'Insulin (Regular) for Hyperkalemia' 'Sodium Polystyrene Sulfonate'\n",
            " 'TraMADOL (Ultram)' 'Lactulose' 'Heparin' 'Sodium Chloride 0.9%  Flush'\n",
            " 'Lactulose' 'Albuterol Inhaler' 'Insulin (Regular) for Hyperkalemia'\n",
            " 'Sodium Polystyrene Sulfonate' 'Calcium Gluconate' 'Dextrose 50%'\n",
            " 'Furosemide' 'Albumin 25% (12.5g / 50mL)' 'Calcium Carbonate'\n",
            " 'Raltegravir' 'Rifaximin' 'Zolpidem Tartrate'\n",
            " 'Fluticasone Propionate 110mcg' 'Albuterol Inhaler' 'Heparin'\n",
            " 'Sodium Chloride 0.9%  Flush' 'Sodium Chloride 0.9%  Flush'\n",
            " 'Calcium Carbonate']\n",
            "med_ids                   : [11, 1, 2, 5, 10, 14, 8, 9, 15, 16, '...(+24 more)']\n",
            "order_type_list           : ['Lab' 'ADT orders' 'Lab' 'General Care' 'General Care' 'Nutrition'\n",
            " 'Medications' 'ADT orders' 'Lab' 'IV therapy' 'Medications'\n",
            " 'General Care' 'General Care' 'General Care' 'Lab' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Radiology' 'Nutrition' 'Nutrition' 'ADT orders' 'Lab' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Cardiology' 'Lab'\n",
            " 'Medications' 'Consults' 'Consults' 'General Care' 'Medications' 'Lab'\n",
            " 'Medications' 'Lab' 'Lab' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'General Care' 'General Care' 'Medications' 'Medications'\n",
            " 'Lab']\n",
            "order_ids                 : [5, 7, 5, 2, 2, 3, 1, 7, 5, 8, '...(+45 more)']\n",
            "\n",
            "\n",
            "--- Row 2 ---\n",
            "gender                    : F\n",
            "race                      : WHITE\n",
            "curr_service              : MED\n",
            "drg_code                  : 207\n",
            "gender_id                 : 0\n",
            "race_id                   : 28\n",
            "service_id                : 7\n",
            "drg_code_id               : 103\n",
            "diagnoses_icd_code_list   : ['45829' '07044' '7994' '2761' '78959' '2767' '3051' 'V08' 'V4986' 'V462'\n",
            " '496' '29680' '5715']\n",
            "diag_ids                  : [13, 14, 15, 4, 2, 11, 8, 7, 16, 10, '...(+3 more)']\n",
            "procedures_icd_code_list  : []\n",
            "hcpcs_cd_list             : []\n",
            "proc_ids                  : []\n",
            "medication_list           : ['Lactulose' 'TraMADOL (Ultram)' 'Sodium Chloride 0.9%  Flush'\n",
            " 'Albumin 25% (12.5g / 50mL)' 'Bisacodyl' 'Calcium Carbonate'\n",
            " 'Docusate Sodium (Liquid)' 'Emtricitabine-Tenofovir (Truvada)'\n",
            " 'Fluticasone Propionate 110mcg' 'Heparin' 'Lactulose' 'Raltegravir'\n",
            " 'Rifaximin' 'Tiotropium Bromide']\n",
            "med_ids                   : [10, 19, 3, 13, 21, 4, 22, 5, 14, 11, '...(+4 more)']\n",
            "order_type_list           : ['Lab' 'ADT orders' 'Lab' 'General Care' 'General Care' 'General Care'\n",
            " 'Nutrition' 'Medications' 'ADT orders' 'Lab' 'General Care'\n",
            " 'General Care' 'General Care' 'Nutrition' 'Medications' 'Medications'\n",
            " 'Medications' 'General Care' 'Medications' 'Respiratory' 'General Care'\n",
            " 'Lab' 'Medications' 'Lab' 'Nutrition' 'Medications' 'Lab' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'Lab' 'ADT orders' 'Lab' 'Medications' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Nutrition' 'Nutrition' 'IV therapy' 'Medications' 'General Care'\n",
            " 'General Care' 'General Care' 'Lab' 'General Care' 'Medications'\n",
            " 'Medications' 'Lab' 'General Care' 'ADT orders' 'Lab' 'Medications'\n",
            " 'Consults' 'Consults' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications']\n",
            "order_ids                 : [5, 7, 5, 2, 2, 2, 3, 1, 7, 5, '...(+61 more)']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def summarize_list(lst, max_len=10):\n",
        "    if lst is None:\n",
        "        return None\n",
        "    if len(lst) <= max_len:\n",
        "        return lst\n",
        "    return lst[:max_len] + [\"...(+{} more)\".format(len(lst) - max_len)]\n",
        "\n",
        "\n",
        "# Output 3 samples\n",
        "cols_to_show = [\n",
        "    \"gender\", \"race\", \"curr_service\", \"drg_code\",\n",
        "    \"gender_id\", \"race_id\", \"service_id\", \"drg_code_id\",\n",
        "    \"diagnoses_icd_code_list\", \"diag_ids\",\n",
        "    \"procedures_icd_code_list\", \"hcpcs_cd_list\", \"proc_ids\",\n",
        "    \"medication_list\", \"med_ids\",\n",
        "    \"order_type_list\", \"order_ids\",\n",
        "]\n",
        "\n",
        "print(\"\\n================ SAMPLE DATA (after encoding) ==============\\n\")\n",
        "\n",
        "for i in range(3):\n",
        "    row = df.iloc[i]\n",
        "    print(f\"--- Row {i} ---\")\n",
        "\n",
        "    for c in cols_to_show:\n",
        "        val = row[c]\n",
        "\n",
        "        # Summarize list\n",
        "        if isinstance(val, list):\n",
        "            val = summarize_list(val)\n",
        "\n",
        "        print(f\"{c:25} : {val}\")\n",
        "\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D0LX9ClZuTkx"
      },
      "outputs": [],
      "source": [
        "class LOSDataset(Dataset):\n",
        "    def __init__(self, df, use_log_target=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.use_log_target = use_log_target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        sample = {\n",
        "            # tabular features\n",
        "            \"age\": float(row[\"anchor_age\"]),\n",
        "            \"gender_id\": int(row[\"gender_id\"]),\n",
        "            \"race_id\": int(row[\"race_id\"]),\n",
        "            \"service_id\": int(row[\"service_id\"]),\n",
        "            \"drg_code_id\": int(row[\"drg_code_id\"]),\n",
        "            \"drg_severity\": float(row[\"drg_severity\"]),\n",
        "            \"drg_mortality\": float(row[\"drg_mortality\"]),\n",
        "\n",
        "            # list ids (codes)\n",
        "            \"diag_ids\": row[\"diag_ids\"],\n",
        "            \"proc_ids\": row[\"proc_ids\"],\n",
        "            \"med_ids\": row[\"med_ids\"],\n",
        "            \"order_ids\": row[\"order_ids\"],\n",
        "        }\n",
        "\n",
        "        # target variable\n",
        "        los = float(row[\"los_hours\"])\n",
        "        if self.use_log_target:\n",
        "            sample[\"target\"] = np.log1p(los)\n",
        "        else:\n",
        "            sample[\"target\"] = los\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZBngU2IuZuA"
      },
      "outputs": [],
      "source": [
        "def los_collate_fn(batch):\n",
        "    B = len(batch)\n",
        "\n",
        "    # ----- Tabular stack -----\n",
        "    age = torch.tensor([b[\"age\"] for b in batch], dtype=torch.float32)\n",
        "    gender_id = torch.tensor([b[\"gender_id\"] for b in batch], dtype=torch.long)\n",
        "    race_id = torch.tensor([b[\"race_id\"] for b in batch], dtype=torch.long)\n",
        "    service_id = torch.tensor([b[\"service_id\"] for b in batch], dtype=torch.long)\n",
        "    drg_code_id = torch.tensor([b[\"drg_code_id\"] for b in batch], dtype=torch.long)\n",
        "    drg_severity = torch.tensor([b[\"drg_severity\"] for b in batch], dtype=torch.float32)\n",
        "    drg_mortality = torch.tensor([b[\"drg_mortality\"] for b in batch], dtype=torch.float32)\n",
        "\n",
        "    target = torch.tensor([b[\"target\"] for b in batch], dtype=torch.float32)\n",
        "\n",
        "    # ----- List-type: diag / proc / med / order -----\n",
        "    def build_bag_inputs(key):\n",
        "        codes_all = []\n",
        "        offsets = [0]\n",
        "        for b in batch:\n",
        "            ids = b[key]\n",
        "            codes_all.extend(ids)\n",
        "            offsets.append(len(codes_all))\n",
        "        if len(codes_all) == 0:\n",
        "            # In case the list is empty for all admissions\n",
        "            codes_tensor = torch.empty(0, dtype=torch.long)\n",
        "        else:\n",
        "            codes_tensor = torch.tensor(codes_all, dtype=torch.long)\n",
        "        offsets_tensor = torch.tensor(offsets, dtype=torch.long)\n",
        "        return codes_tensor, offsets_tensor\n",
        "\n",
        "    diag_codes, diag_offsets = build_bag_inputs(\"diag_ids\")\n",
        "    proc_codes, proc_offsets = build_bag_inputs(\"proc_ids\")\n",
        "    med_codes, med_offsets = build_bag_inputs(\"med_ids\")\n",
        "    order_codes, order_offsets = build_bag_inputs(\"order_ids\")\n",
        "\n",
        "    batch_out = {\n",
        "        \"age\": age,\n",
        "        \"gender_id\": gender_id,\n",
        "        \"race_id\": race_id,\n",
        "        \"service_id\": service_id,\n",
        "        \"drg_code_id\": drg_code_id,\n",
        "        \"drg_severity\": drg_severity,\n",
        "        \"drg_mortality\": drg_mortality,\n",
        "        \"diag_codes\": diag_codes,\n",
        "        \"diag_offsets\": diag_offsets,\n",
        "        \"proc_codes\": proc_codes,\n",
        "        \"proc_offsets\": proc_offsets,\n",
        "        \"med_codes\": med_codes,\n",
        "        \"med_offsets\": med_offsets,\n",
        "        \"order_codes\": order_codes,\n",
        "        \"order_offsets\": order_offsets,\n",
        "        \"target\": target,\n",
        "    }\n",
        "\n",
        "    return batch_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmEgyvbPuaOO",
        "outputId": "1beaac38-9e13-4452-f39a-5e66210ebd8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#train: 297362, #val: 63720, #test: 63721\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "import torch\n",
        "\n",
        "# Generate Dataset\n",
        "dataset = LOSDataset(df, use_log_target=True)\n",
        "\n",
        "n_total = len(dataset)\n",
        "n_train = int(n_total * 0.7)\n",
        "n_val = int(n_total * 0.15)\n",
        "n_test = n_total - n_train - n_val\n",
        "\n",
        "# Use generator for reproducibility (optional)\n",
        "g = torch.Generator().manual_seed(42)\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test], generator=g)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    collate_fn=los_collate_fn,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    collate_fn=los_collate_fn,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    collate_fn=los_collate_fn,\n",
        ")\n",
        "\n",
        "print(f\"#train: {len(train_ds)}, #val: {len(val_ds)}, #test: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "df441b0e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    # ----- Tabular categorical vocab sizes -----\n",
        "    num_genders: int          # e.g. {\"M\", \"F\"} -> 2\n",
        "    num_races: int            # unique race categories\n",
        "    num_services: int         # curr_service (MED, ORTHO, ...)\n",
        "    num_drg_codes: int        # drg_code vocab size\n",
        "\n",
        "    # ----- Code vocab sizes -----\n",
        "    diag_vocab_size: int      # diagnoses_icd_code_list vocab\n",
        "    proc_vocab_size: int      # procedures_icd_code_list + hcpcs_cd_list unified vocab\n",
        "    med_vocab_size: int       # medication_list vocab\n",
        "    order_vocab_size: int     # order_type_list vocab\n",
        "\n",
        "    # ----- Embedding dimensions -----\n",
        "    emb_dim_gender: int = 4\n",
        "    emb_dim_race: int = 8\n",
        "    emb_dim_service: int = 8\n",
        "    emb_dim_drg: int = 16\n",
        "\n",
        "    emb_dim_diag: int = 32\n",
        "    emb_dim_proc: int = 32\n",
        "    emb_dim_med: int = 32\n",
        "    emb_dim_order: int = 16\n",
        "\n",
        "    # ----- Hidden dimensions -----\n",
        "    tabular_hidden_dim: int = 64\n",
        "    diag_hidden_dim: int = 64\n",
        "    proc_hidden_dim: int = 64\n",
        "    med_hidden_dim: int = 64\n",
        "    order_hidden_dim: int = 32\n",
        "\n",
        "    fusion_hidden_dim: int = 128\n",
        "    dropout: float = 0.2\n",
        "\n",
        "\n",
        "class MultiModalLOSModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-branch model for LOS prediction.\n",
        "\n",
        "    Branches:\n",
        "      - Tabular branch: age, gender, race, drg, severity, mortality, curr_service\n",
        "      - Diagnostic branch: diagnoses_icd_code_list\n",
        "      - Procedure branch: procedures_icd_code_list + hcpcs_cd_list\n",
        "      - Medication branch: medication_list\n",
        "      - Order-type branch: order_type_list\n",
        "\n",
        "    Fusion:\n",
        "      concat([h_tab, h_diag, h_proc, h_med, h_order]) -> Attention MLP -> Fusion MLP -> scalar LOS prediction\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # ------------- Tabular embeddings -------------\n",
        "        self.gender_emb = nn.Embedding(cfg.num_genders, cfg.emb_dim_gender)\n",
        "        self.race_emb = nn.Embedding(cfg.num_races, cfg.emb_dim_race)\n",
        "        self.service_emb = nn.Embedding(cfg.num_services, cfg.emb_dim_service)\n",
        "        self.drg_emb = nn.Embedding(cfg.num_drg_codes, cfg.emb_dim_drg)\n",
        "\n",
        "        # Tabular MLP input dim:\n",
        "        #   age(1) + severity(1) + mortality(1)\n",
        "        # + gender_emb + race_emb + service_emb + drg_emb\n",
        "        tab_in_dim = (\n",
        "            3\n",
        "            + cfg.emb_dim_gender\n",
        "            + cfg.emb_dim_race\n",
        "            + cfg.emb_dim_service\n",
        "            + cfg.emb_dim_drg\n",
        "        )\n",
        "\n",
        "        self.tabular_mlp = nn.Sequential(\n",
        "            nn.Linear(tab_in_dim, cfg.tabular_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(cfg.tabular_hidden_dim, cfg.tabular_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # ------------- Code embeddings (EmbeddingBag: bag-of-codes) -------------\n",
        "        self.diag_emb = nn.EmbeddingBag(\n",
        "            cfg.diag_vocab_size, cfg.emb_dim_diag,\n",
        "            mode=\"mean\", include_last_offset=True\n",
        "        )\n",
        "        self.proc_emb = nn.EmbeddingBag(\n",
        "            cfg.proc_vocab_size, cfg.emb_dim_proc,\n",
        "            mode=\"mean\", include_last_offset=True\n",
        "        )\n",
        "        self.med_emb = nn.EmbeddingBag(\n",
        "            cfg.med_vocab_size, cfg.emb_dim_med,\n",
        "            mode=\"mean\", include_last_offset=True\n",
        "        )\n",
        "        self.order_emb = nn.EmbeddingBag(\n",
        "            cfg.order_vocab_size, cfg.emb_dim_order,\n",
        "            mode=\"mean\", include_last_offset=True\n",
        "        )\n",
        "\n",
        "        # Small projection MLP for each branch\n",
        "        self.diag_mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.emb_dim_diag, cfg.diag_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "        )\n",
        "        self.proc_mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.emb_dim_proc, cfg.proc_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "        )\n",
        "        self.med_mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.emb_dim_med, cfg.med_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "        )\n",
        "        self.order_mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.emb_dim_order, cfg.order_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "        )\n",
        "\n",
        "        # ------------- Attention mechanism -------------\n",
        "        # fusion_in_dim is the concatenated size of all branch hidden dimensions\n",
        "        fusion_in_dim = (\n",
        "            cfg.tabular_hidden_dim\n",
        "            + cfg.diag_hidden_dim\n",
        "            + cfg.proc_hidden_dim\n",
        "            + cfg.med_hidden_dim\n",
        "            + cfg.order_hidden_dim\n",
        "        )\n",
        "        self.attention_mlp = nn.Linear(fusion_in_dim, 5) # 5 branches\n",
        "\n",
        "        # ------------- Fusion MLP -------------\n",
        "        # Fusion MLP input dim is still the sum of hidden dims (after scaling)\n",
        "        self.fusion_mlp = nn.Sequential(\n",
        "            nn.Linear(fusion_in_dim, cfg.fusion_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(cfg.fusion_hidden_dim, cfg.fusion_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "        )\n",
        "\n",
        "        # Final regression output layer (LOS prediction)\n",
        "        self.out = nn.Linear(cfg.fusion_hidden_dim, 1)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Forward: batch input format\n",
        "    # ------------------------------------------------------------------\n",
        "    def forward(\n",
        "        self,\n",
        "        # ----- Tabular -----\n",
        "        age,               # (B,) float tensor (anchor_age or normalized)\n",
        "        gender_idx,        # (B,) long tensor\n",
        "        race_idx,          # (B,) long tensor\n",
        "        service_idx,       # (B,) long tensor (curr_service)\n",
        "        drg_code_idx,      # (B,) long tensor\n",
        "        drg_severity,      # (B,) float or long (recommend normalizing to float)\n",
        "        drg_mortality,     # (B,) float or long\n",
        "\n",
        "        # ----- Diagnoses (EmbeddingBag) -----\n",
        "        diag_codes,        # (N_diag_codes,) long tensor (flattened)\n",
        "        diag_offsets,      # (B+1,) long tensor, EmbeddingBag offset\n",
        "\n",
        "        # ----- Procedures (EmbeddingBag) -----\n",
        "        proc_codes,        # (N_proc_codes,) long tensor\n",
        "        proc_offsets,      # (B+1,) long tensor\n",
        "\n",
        "        # ----- Medications (EmbeddingBag) -----\n",
        "        med_codes,         # (N_med_codes,) long tensor\n",
        "        med_offsets,       # (B+1,) long tensor\n",
        "\n",
        "        # ----- Order types (EmbeddingBag) -----\n",
        "        order_codes,       # (N_order_codes,) long tensor\n",
        "        order_offsets,     # (B+1,) long tensor\n",
        "    ):\n",
        "        # --------- 1. Tabular branch ---------\n",
        "        # Embeddings\n",
        "        g_emb = self.gender_emb(gender_idx)   # (B, emb_dim_gender)\n",
        "        r_emb = self.race_emb(race_idx)       # (B, emb_dim_race)\n",
        "        s_emb = self.service_emb(service_idx) # (B, emb_dim_service)\n",
        "        d_emb = self.drg_emb(drg_code_idx)    # (B, emb_dim_drg)\n",
        "\n",
        "        # Cast continuous features to float\n",
        "        age = age.float().unsqueeze(-1)                 # (B, 1)\n",
        "        sev = drg_severity.float().unsqueeze(-1)        # (B, 1)\n",
        "        mort = drg_mortality.float().unsqueeze(-1)      # (B, 1)\n",
        "\n",
        "        tabular_feat = torch.cat(\n",
        "            [age, sev, mort, g_emb, r_emb, s_emb, d_emb],\n",
        "            dim=-1\n",
        "        )  # (B, tab_in_dim)\n",
        "\n",
        "        h_tab = self.tabular_mlp(tabular_feat)  # (B, tabular_hidden_dim)\n",
        "\n",
        "        # --------- 2. Diagnostic branch ---------\n",
        "        diag_bag = self.diag_emb(diag_codes, diag_offsets)  # (B, emb_dim_diag)\n",
        "        h_diag = self.diag_mlp(diag_bag)  # (B, diag_hidden_dim)\n",
        "\n",
        "        # --------- 3. Procedure branch ---------\n",
        "        proc_bag = self.proc_emb(proc_codes, proc_offsets)  # (B, emb_dim_proc)\n",
        "        h_proc = self.proc_mlp(proc_bag)  # (B, proc_hidden_dim)\n",
        "\n",
        "        # --------- 4. Medication branch ---------\n",
        "        med_bag = self.med_emb(med_codes, med_offsets)  # (B, emb_dim_med)\n",
        "        h_med = self.med_mlp(med_bag)  # (B, med_hidden_dim)\n",
        "\n",
        "        # --------- 5. Order-type branch ---------\n",
        "        order_bag = self.order_emb(order_codes, order_offsets)  # (B, emb_dim_order)\n",
        "        h_order = self.order_mlp(order_bag)  # (B, order_hidden_dim)\n",
        "\n",
        "        # --------- 6. Apply Self-Attention Mechanism ---------\n",
        "        # Concatenate hidden representations for attention input\n",
        "        h_pre_attention = torch.cat([h_tab, h_diag, h_proc, h_med, h_order], dim=-1)\n",
        "\n",
        "        # Compute raw attention scores\n",
        "        alpha_raw = self.attention_mlp(h_pre_attention) # (B, 5)\n",
        "\n",
        "        # Apply softmax to get normalized attention weights\n",
        "        alpha_weights = F.softmax(alpha_raw, dim=-1) # (B, 5)\n",
        "\n",
        "        # Split alpha_weights into individual branch weights\n",
        "        alpha_tab = alpha_weights[:, 0].unsqueeze(-1)\n",
        "        alpha_diag = alpha_weights[:, 1].unsqueeze(-1)\n",
        "        alpha_proc = alpha_weights[:, 2].unsqueeze(-1)\n",
        "        alpha_med = alpha_weights[:, 3].unsqueeze(-1)\n",
        "        alpha_order = alpha_weights[:, 4].unsqueeze(-1)\n",
        "\n",
        "        # Scale each branch's hidden representation by its attention weight\n",
        "        h_tab_scaled = h_tab * alpha_tab\n",
        "        h_diag_scaled = h_diag * alpha_diag\n",
        "        h_proc_scaled = h_proc * alpha_proc\n",
        "        h_med_scaled = h_med * alpha_med\n",
        "        h_order_scaled = h_order * alpha_order\n",
        "\n",
        "        # --------- 7. Fusion ---------\n",
        "        h = torch.cat([h_tab_scaled, h_diag_scaled, h_proc_scaled, h_med_scaled, h_order_scaled], dim=-1)\n",
        "        h = self.fusion_mlp(h)\n",
        "        out = self.out(h).squeeze(-1)  # (B,)\n",
        "\n",
        "        # out = Predicted LOS (can directly use hours, or design to predict log1p)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2de54af",
        "outputId": "30e79f3d-c5a1-4a3b-f7bf-2a15a6cc0855"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "========== Epoch 1/20 ==========\n",
            "  [Epoch 1 | Step 0/1162] AvgTrainLoss=22.0654\n",
            "  [Epoch 1 | Step 100/1162] AvgTrainLoss=3.8523\n",
            "  [Epoch 1 | Step 200/1162] AvgTrainLoss=2.4147\n",
            "  [Epoch 1 | Step 300/1162] AvgTrainLoss=1.8707\n",
            "  [Epoch 1 | Step 400/1162] AvgTrainLoss=1.5802\n",
            "  [Epoch 1 | Step 500/1162] AvgTrainLoss=1.3983\n",
            "  [Epoch 1 | Step 600/1162] AvgTrainLoss=1.2692\n",
            "  [Epoch 1 | Step 700/1162] AvgTrainLoss=1.1789\n",
            "  [Epoch 1 | Step 800/1162] AvgTrainLoss=1.1078\n",
            "  [Epoch 1 | Step 900/1162] AvgTrainLoss=1.0511\n",
            "  [Epoch 1 | Step 1000/1162] AvgTrainLoss=1.0012\n",
            "  [Epoch 1 | Step 1100/1162] AvgTrainLoss=0.9592\n",
            "[Epoch 001] train_loss(log-MSE)=0.9361 | val_loss(log-MSE)=0.4046 | val_MAE(hours)=73.24\n",
            "  ↳ Best model updated, saved to los_multibranch_attention_best.pt\n",
            "\n",
            "========== Epoch 2/20 ==========\n",
            "  [Epoch 2 | Step 0/1162] AvgTrainLoss=0.4641\n",
            "  [Epoch 2 | Step 100/1162] AvgTrainLoss=0.4890\n",
            "  [Epoch 2 | Step 200/1162] AvgTrainLoss=0.4801\n",
            "  [Epoch 2 | Step 300/1162] AvgTrainLoss=0.4727\n",
            "  [Epoch 2 | Step 400/1162] AvgTrainLoss=0.4672\n",
            "  [Epoch 2 | Step 500/1162] AvgTrainLoss=0.4605\n",
            "  [Epoch 2 | Step 600/1162] AvgTrainLoss=0.4557\n",
            "  [Epoch 2 | Step 700/1162] AvgTrainLoss=0.4520\n",
            "  [Epoch 2 | Step 800/1162] AvgTrainLoss=0.4482\n",
            "  [Epoch 2 | Step 900/1162] AvgTrainLoss=0.4448\n",
            "  [Epoch 2 | Step 1000/1162] AvgTrainLoss=0.4414\n",
            "  [Epoch 2 | Step 1100/1162] AvgTrainLoss=0.4381\n",
            "[Epoch 002] train_loss(log-MSE)=0.4357 | val_loss(log-MSE)=0.2935 | val_MAE(hours)=62.91\n",
            "  ↳ Best model updated, saved to los_multibranch_attention_best.pt\n",
            "\n",
            "========== Epoch 3/20 ==========\n",
            "  [Epoch 3 | Step 0/1162] AvgTrainLoss=0.3670\n",
            "  [Epoch 3 | Step 100/1162] AvgTrainLoss=0.3824\n",
            "  [Epoch 3 | Step 200/1162] AvgTrainLoss=0.3840\n",
            "  [Epoch 3 | Step 300/1162] AvgTrainLoss=0.3835\n",
            "  [Epoch 3 | Step 400/1162] AvgTrainLoss=0.3822\n",
            "  [Epoch 3 | Step 500/1162] AvgTrainLoss=0.3806\n",
            "  [Epoch 3 | Step 600/1162] AvgTrainLoss=0.3795\n",
            "  [Epoch 3 | Step 700/1162] AvgTrainLoss=0.3789\n",
            "  [Epoch 3 | Step 800/1162] AvgTrainLoss=0.3771\n",
            "  [Epoch 3 | Step 900/1162] AvgTrainLoss=0.3748\n",
            "  [Epoch 3 | Step 1000/1162] AvgTrainLoss=0.3734\n",
            "  [Epoch 3 | Step 1100/1162] AvgTrainLoss=0.3718\n",
            "[Epoch 003] train_loss(log-MSE)=0.3708 | val_loss(log-MSE)=0.2744 | val_MAE(hours)=61.30\n",
            "  ↳ Best model updated, saved to los_multibranch_attention_best.pt\n",
            "\n",
            "========== Epoch 4/20 ==========\n",
            "  [Epoch 4 | Step 0/1162] AvgTrainLoss=0.3412\n",
            "  [Epoch 4 | Step 100/1162] AvgTrainLoss=0.3449\n",
            "  [Epoch 4 | Step 200/1162] AvgTrainLoss=0.3447\n",
            "  [Epoch 4 | Step 300/1162] AvgTrainLoss=0.3437\n",
            "  [Epoch 4 | Step 400/1162] AvgTrainLoss=0.3431\n",
            "  [Epoch 4 | Step 500/1162] AvgTrainLoss=0.3425\n",
            "  [Epoch 4 | Step 600/1162] AvgTrainLoss=0.3419\n",
            "  [Epoch 4 | Step 700/1162] AvgTrainLoss=0.3415\n",
            "  [Epoch 4 | Step 800/1162] AvgTrainLoss=0.3411\n",
            "  [Epoch 4 | Step 900/1162] AvgTrainLoss=0.3411\n",
            "  [Epoch 4 | Step 1000/1162] AvgTrainLoss=0.3404\n",
            "  [Epoch 4 | Step 1100/1162] AvgTrainLoss=0.3401\n",
            "[Epoch 004] train_loss(log-MSE)=0.3399 | val_loss(log-MSE)=0.2739 | val_MAE(hours)=61.13\n",
            "  ↳ Best model updated, saved to los_multibranch_attention_best.pt\n",
            "\n",
            "========== Epoch 5/20 ==========\n",
            "  [Epoch 5 | Step 0/1162] AvgTrainLoss=0.3205\n",
            "  [Epoch 5 | Step 100/1162] AvgTrainLoss=0.3157\n",
            "  [Epoch 5 | Step 200/1162] AvgTrainLoss=0.3166\n",
            "  [Epoch 5 | Step 300/1162] AvgTrainLoss=0.3176\n",
            "  [Epoch 5 | Step 400/1162] AvgTrainLoss=0.3173\n",
            "  [Epoch 5 | Step 500/1162] AvgTrainLoss=0.3169\n",
            "  [Epoch 5 | Step 600/1162] AvgTrainLoss=0.3167\n",
            "  [Epoch 5 | Step 700/1162] AvgTrainLoss=0.3169\n",
            "  [Epoch 5 | Step 800/1162] AvgTrainLoss=0.3174\n",
            "  [Epoch 5 | Step 900/1162] AvgTrainLoss=0.3168\n",
            "  [Epoch 5 | Step 1000/1162] AvgTrainLoss=0.3163\n",
            "  [Epoch 5 | Step 1100/1162] AvgTrainLoss=0.3160\n",
            "[Epoch 005] train_loss(log-MSE)=0.3156 | val_loss(log-MSE)=0.2606 | val_MAE(hours)=59.69\n",
            "  ↳ Best model updated, saved to los_multibranch_attention_best.pt\n",
            "\n",
            "========== Epoch 6/20 ==========\n",
            "  [Epoch 6 | Step 0/1162] AvgTrainLoss=0.2537\n",
            "  [Epoch 6 | Step 100/1162] AvgTrainLoss=0.2951\n",
            "  [Epoch 6 | Step 200/1162] AvgTrainLoss=0.2967\n",
            "  [Epoch 6 | Step 300/1162] AvgTrainLoss=0.2950\n",
            "  [Epoch 6 | Step 400/1162] AvgTrainLoss=0.2942\n",
            "  [Epoch 6 | Step 500/1162] AvgTrainLoss=0.2944\n",
            "  [Epoch 6 | Step 600/1162] AvgTrainLoss=0.2946\n",
            "  [Epoch 6 | Step 700/1162] AvgTrainLoss=0.2943\n",
            "  [Epoch 6 | Step 800/1162] AvgTrainLoss=0.2952\n",
            "  [Epoch 6 | Step 900/1162] AvgTrainLoss=0.2946\n",
            "  [Epoch 6 | Step 1000/1162] AvgTrainLoss=0.2942\n",
            "  [Epoch 6 | Step 1100/1162] AvgTrainLoss=0.2946\n",
            "[Epoch 006] train_loss(log-MSE)=0.2947 | val_loss(log-MSE)=0.2843 | val_MAE(hours)=62.63\n",
            "\n",
            "========== Epoch 7/20 ==========\n",
            "  [Epoch 7 | Step 0/1162] AvgTrainLoss=0.3093\n",
            "  [Epoch 7 | Step 100/1162] AvgTrainLoss=0.2814\n",
            "  [Epoch 7 | Step 200/1162] AvgTrainLoss=0.2790\n",
            "  [Epoch 7 | Step 300/1162] AvgTrainLoss=0.2787\n",
            "  [Epoch 7 | Step 400/1162] AvgTrainLoss=0.2795\n",
            "  [Epoch 7 | Step 500/1162] AvgTrainLoss=0.2794\n",
            "  [Epoch 7 | Step 600/1162] AvgTrainLoss=0.2793\n",
            "  [Epoch 7 | Step 700/1162] AvgTrainLoss=0.2797\n",
            "  [Epoch 7 | Step 800/1162] AvgTrainLoss=0.2794\n",
            "  [Epoch 7 | Step 900/1162] AvgTrainLoss=0.2793\n",
            "  [Epoch 7 | Step 1000/1162] AvgTrainLoss=0.2791\n",
            "  [Epoch 7 | Step 1100/1162] AvgTrainLoss=0.2796\n",
            "[Epoch 007] train_loss(log-MSE)=0.2791 | val_loss(log-MSE)=0.2549 | val_MAE(hours)=58.85\n",
            "  ↳ Best model updated, saved to los_multibranch_attention_best.pt\n",
            "\n",
            "========== Epoch 8/20 ==========\n",
            "  [Epoch 8 | Step 0/1162] AvgTrainLoss=0.2487\n",
            "  [Epoch 8 | Step 100/1162] AvgTrainLoss=0.2611\n",
            "  [Epoch 8 | Step 200/1162] AvgTrainLoss=0.2593\n",
            "  [Epoch 8 | Step 300/1162] AvgTrainLoss=0.2605\n",
            "  [Epoch 8 | Step 400/1162] AvgTrainLoss=0.2609\n",
            "  [Epoch 8 | Step 500/1162] AvgTrainLoss=0.2630\n",
            "  [Epoch 8 | Step 600/1162] AvgTrainLoss=0.2631\n",
            "  [Epoch 8 | Step 700/1162] AvgTrainLoss=0.2634\n",
            "  [Epoch 8 | Step 800/1162] AvgTrainLoss=0.2641\n",
            "  [Epoch 8 | Step 900/1162] AvgTrainLoss=0.2646\n",
            "  [Epoch 8 | Step 1000/1162] AvgTrainLoss=0.2650\n",
            "  [Epoch 8 | Step 1100/1162] AvgTrainLoss=0.2652\n",
            "[Epoch 008] train_loss(log-MSE)=0.2655 | val_loss(log-MSE)=0.2559 | val_MAE(hours)=59.22\n",
            "\n",
            "========== Epoch 9/20 ==========\n",
            "  [Epoch 9 | Step 0/1162] AvgTrainLoss=0.2155\n",
            "  [Epoch 9 | Step 100/1162] AvgTrainLoss=0.2515\n",
            "  [Epoch 9 | Step 200/1162] AvgTrainLoss=0.2492\n",
            "  [Epoch 9 | Step 300/1162] AvgTrainLoss=0.2505\n",
            "  [Epoch 9 | Step 400/1162] AvgTrainLoss=0.2518\n",
            "  [Epoch 9 | Step 500/1162] AvgTrainLoss=0.2533\n",
            "  [Epoch 9 | Step 600/1162] AvgTrainLoss=0.2538\n",
            "  [Epoch 9 | Step 700/1162] AvgTrainLoss=0.2540\n",
            "  [Epoch 9 | Step 800/1162] AvgTrainLoss=0.2542\n",
            "  [Epoch 9 | Step 900/1162] AvgTrainLoss=0.2548\n",
            "  [Epoch 9 | Step 1000/1162] AvgTrainLoss=0.2550\n",
            "  [Epoch 9 | Step 1100/1162] AvgTrainLoss=0.2546\n",
            "[Epoch 009] train_loss(log-MSE)=0.2549 | val_loss(log-MSE)=0.2520 | val_MAE(hours)=58.57\n",
            "  ↳ Best model updated, saved to los_multibranch_attention_best.pt\n",
            "\n",
            "========== Epoch 10/20 ==========\n",
            "  [Epoch 10 | Step 0/1162] AvgTrainLoss=0.2097\n",
            "  [Epoch 10 | Step 100/1162] AvgTrainLoss=0.2464\n",
            "  [Epoch 10 | Step 200/1162] AvgTrainLoss=0.2412\n",
            "  [Epoch 10 | Step 300/1162] AvgTrainLoss=0.2409\n",
            "  [Epoch 10 | Step 400/1162] AvgTrainLoss=0.2416\n",
            "  [Epoch 10 | Step 500/1162] AvgTrainLoss=0.2420\n",
            "  [Epoch 10 | Step 600/1162] AvgTrainLoss=0.2419\n",
            "  [Epoch 10 | Step 700/1162] AvgTrainLoss=0.2423\n",
            "  [Epoch 10 | Step 800/1162] AvgTrainLoss=0.2422\n",
            "  [Epoch 10 | Step 900/1162] AvgTrainLoss=0.2429\n",
            "  [Epoch 10 | Step 1000/1162] AvgTrainLoss=0.2434\n",
            "  [Epoch 10 | Step 1100/1162] AvgTrainLoss=0.2435\n",
            "[Epoch 010] train_loss(log-MSE)=0.2433 | val_loss(log-MSE)=0.2523 | val_MAE(hours)=58.69\n",
            "\n",
            "========== Epoch 11/20 ==========\n",
            "  [Epoch 11 | Step 0/1162] AvgTrainLoss=0.2105\n",
            "  [Epoch 11 | Step 100/1162] AvgTrainLoss=0.2306\n",
            "  [Epoch 11 | Step 200/1162] AvgTrainLoss=0.2299\n",
            "  [Epoch 11 | Step 300/1162] AvgTrainLoss=0.2296\n",
            "  [Epoch 11 | Step 400/1162] AvgTrainLoss=0.2310\n",
            "  [Epoch 11 | Step 500/1162] AvgTrainLoss=0.2312\n",
            "  [Epoch 11 | Step 600/1162] AvgTrainLoss=0.2320\n",
            "  [Epoch 11 | Step 700/1162] AvgTrainLoss=0.2326\n",
            "  [Epoch 11 | Step 800/1162] AvgTrainLoss=0.2327\n",
            "  [Epoch 11 | Step 900/1162] AvgTrainLoss=0.2330\n",
            "  [Epoch 11 | Step 1000/1162] AvgTrainLoss=0.2333\n",
            "  [Epoch 11 | Step 1100/1162] AvgTrainLoss=0.2337\n",
            "[Epoch 011] train_loss(log-MSE)=0.2340 | val_loss(log-MSE)=0.2526 | val_MAE(hours)=58.76\n",
            "\n",
            "========== Epoch 12/20 ==========\n",
            "  [Epoch 12 | Step 0/1162] AvgTrainLoss=0.2218\n",
            "  [Epoch 12 | Step 100/1162] AvgTrainLoss=0.2210\n",
            "  [Epoch 12 | Step 200/1162] AvgTrainLoss=0.2198\n",
            "  [Epoch 12 | Step 300/1162] AvgTrainLoss=0.2220\n",
            "  [Epoch 12 | Step 400/1162] AvgTrainLoss=0.2213\n",
            "  [Epoch 12 | Step 500/1162] AvgTrainLoss=0.2221\n",
            "  [Epoch 12 | Step 600/1162] AvgTrainLoss=0.2224\n",
            "  [Epoch 12 | Step 700/1162] AvgTrainLoss=0.2225\n",
            "  [Epoch 12 | Step 800/1162] AvgTrainLoss=0.2235\n",
            "  [Epoch 12 | Step 900/1162] AvgTrainLoss=0.2243\n",
            "  [Epoch 12 | Step 1000/1162] AvgTrainLoss=0.2243\n",
            "  [Epoch 12 | Step 1100/1162] AvgTrainLoss=0.2250\n",
            "[Epoch 012] train_loss(log-MSE)=0.2252 | val_loss(log-MSE)=0.2560 | val_MAE(hours)=59.66\n",
            "\n",
            "========== Epoch 13/20 ==========\n",
            "  [Epoch 13 | Step 0/1162] AvgTrainLoss=0.2132\n",
            "  [Epoch 13 | Step 100/1162] AvgTrainLoss=0.2107\n",
            "  [Epoch 13 | Step 200/1162] AvgTrainLoss=0.2109\n",
            "  [Epoch 13 | Step 300/1162] AvgTrainLoss=0.2130\n",
            "  [Epoch 13 | Step 400/1162] AvgTrainLoss=0.2135\n",
            "  [Epoch 13 | Step 500/1162] AvgTrainLoss=0.2151\n",
            "  [Epoch 13 | Step 600/1162] AvgTrainLoss=0.2152\n",
            "  [Epoch 13 | Step 700/1162] AvgTrainLoss=0.2156\n",
            "  [Epoch 13 | Step 800/1162] AvgTrainLoss=0.2162\n",
            "  [Epoch 13 | Step 900/1162] AvgTrainLoss=0.2167\n",
            "  [Epoch 13 | Step 1000/1162] AvgTrainLoss=0.2173\n",
            "  [Epoch 13 | Step 1100/1162] AvgTrainLoss=0.2176\n",
            "[Epoch 013] train_loss(log-MSE)=0.2178 | val_loss(log-MSE)=0.2559 | val_MAE(hours)=59.90\n",
            "\n",
            "========== Epoch 14/20 ==========\n",
            "  [Epoch 14 | Step 0/1162] AvgTrainLoss=0.2276\n",
            "  [Epoch 14 | Step 100/1162] AvgTrainLoss=0.2021\n",
            "  [Epoch 14 | Step 200/1162] AvgTrainLoss=0.2018\n",
            "  [Epoch 14 | Step 300/1162] AvgTrainLoss=0.2045\n",
            "  [Epoch 14 | Step 400/1162] AvgTrainLoss=0.2055\n",
            "  [Epoch 14 | Step 500/1162] AvgTrainLoss=0.2067\n",
            "  [Epoch 14 | Step 600/1162] AvgTrainLoss=0.2078\n",
            "  [Epoch 14 | Step 700/1162] AvgTrainLoss=0.2079\n",
            "  [Epoch 14 | Step 800/1162] AvgTrainLoss=0.2086\n",
            "  [Epoch 14 | Step 900/1162] AvgTrainLoss=0.2090\n",
            "  [Epoch 14 | Step 1000/1162] AvgTrainLoss=0.2094\n",
            "  [Epoch 14 | Step 1100/1162] AvgTrainLoss=0.2098\n",
            "[Epoch 014] train_loss(log-MSE)=0.2103 | val_loss(log-MSE)=0.2591 | val_MAE(hours)=59.92\n",
            "\n",
            "========== Epoch 15/20 ==========\n",
            "  [Epoch 15 | Step 0/1162] AvgTrainLoss=0.2031\n",
            "  [Epoch 15 | Step 100/1162] AvgTrainLoss=0.1993\n",
            "  [Epoch 15 | Step 200/1162] AvgTrainLoss=0.1991\n",
            "  [Epoch 15 | Step 300/1162] AvgTrainLoss=0.2001\n",
            "  [Epoch 15 | Step 400/1162] AvgTrainLoss=0.2007\n",
            "  [Epoch 15 | Step 500/1162] AvgTrainLoss=0.2005\n",
            "  [Epoch 15 | Step 600/1162] AvgTrainLoss=0.2018\n",
            "  [Epoch 15 | Step 700/1162] AvgTrainLoss=0.2022\n",
            "  [Epoch 15 | Step 800/1162] AvgTrainLoss=0.2023\n",
            "  [Epoch 15 | Step 900/1162] AvgTrainLoss=0.2028\n",
            "  [Epoch 15 | Step 1000/1162] AvgTrainLoss=0.2029\n",
            "  [Epoch 15 | Step 1100/1162] AvgTrainLoss=0.2036\n",
            "[Epoch 015] train_loss(log-MSE)=0.2040 | val_loss(log-MSE)=0.2518 | val_MAE(hours)=58.76\n",
            "  ↳ Best model updated, saved to los_multibranch_attention_best.pt\n",
            "\n",
            "========== Epoch 16/20 ==========\n",
            "  [Epoch 16 | Step 0/1162] AvgTrainLoss=0.1598\n",
            "  [Epoch 16 | Step 100/1162] AvgTrainLoss=0.1900\n",
            "  [Epoch 16 | Step 200/1162] AvgTrainLoss=0.1916\n",
            "  [Epoch 16 | Step 300/1162] AvgTrainLoss=0.1921\n",
            "  [Epoch 16 | Step 400/1162] AvgTrainLoss=0.1928\n",
            "  [Epoch 16 | Step 500/1162] AvgTrainLoss=0.1935\n",
            "  [Epoch 16 | Step 600/1162] AvgTrainLoss=0.1947\n",
            "  [Epoch 16 | Step 700/1162] AvgTrainLoss=0.1954\n",
            "  [Epoch 16 | Step 800/1162] AvgTrainLoss=0.1961\n",
            "  [Epoch 16 | Step 900/1162] AvgTrainLoss=0.1969\n",
            "  [Epoch 16 | Step 1000/1162] AvgTrainLoss=0.1975\n",
            "  [Epoch 16 | Step 1100/1162] AvgTrainLoss=0.1979\n",
            "[Epoch 016] train_loss(log-MSE)=0.1982 | val_loss(log-MSE)=0.2533 | val_MAE(hours)=58.87\n",
            "\n",
            "========== Epoch 17/20 ==========\n",
            "  [Epoch 17 | Step 0/1162] AvgTrainLoss=0.1739\n",
            "  [Epoch 17 | Step 100/1162] AvgTrainLoss=0.1816\n",
            "  [Epoch 17 | Step 200/1162] AvgTrainLoss=0.1850\n",
            "  [Epoch 17 | Step 300/1162] AvgTrainLoss=0.1864\n",
            "  [Epoch 17 | Step 400/1162] AvgTrainLoss=0.1879\n",
            "  [Epoch 17 | Step 500/1162] AvgTrainLoss=0.1887\n",
            "  [Epoch 17 | Step 600/1162] AvgTrainLoss=0.1894\n",
            "  [Epoch 17 | Step 700/1162] AvgTrainLoss=0.1902\n",
            "  [Epoch 17 | Step 800/1162] AvgTrainLoss=0.1914\n",
            "  [Epoch 17 | Step 900/1162] AvgTrainLoss=0.1915\n",
            "  [Epoch 17 | Step 1000/1162] AvgTrainLoss=0.1922\n",
            "  [Epoch 17 | Step 1100/1162] AvgTrainLoss=0.1928\n",
            "[Epoch 017] train_loss(log-MSE)=0.1931 | val_loss(log-MSE)=0.2550 | val_MAE(hours)=59.14\n",
            "\n",
            "========== Epoch 18/20 ==========\n",
            "  [Epoch 18 | Step 0/1162] AvgTrainLoss=0.1749\n",
            "  [Epoch 18 | Step 100/1162] AvgTrainLoss=0.1821\n",
            "  [Epoch 18 | Step 200/1162] AvgTrainLoss=0.1832\n",
            "  [Epoch 18 | Step 300/1162] AvgTrainLoss=0.1829\n",
            "  [Epoch 18 | Step 400/1162] AvgTrainLoss=0.1831\n",
            "  [Epoch 18 | Step 500/1162] AvgTrainLoss=0.1839\n",
            "  [Epoch 18 | Step 600/1162] AvgTrainLoss=0.1843\n",
            "  [Epoch 18 | Step 700/1162] AvgTrainLoss=0.1849\n",
            "  [Epoch 18 | Step 800/1162] AvgTrainLoss=0.1855\n",
            "  [Epoch 18 | Step 900/1162] AvgTrainLoss=0.1866\n",
            "  [Epoch 18 | Step 1000/1162] AvgTrainLoss=0.1874\n",
            "  [Epoch 18 | Step 1100/1162] AvgTrainLoss=0.1881\n",
            "[Epoch 018] train_loss(log-MSE)=0.1885 | val_loss(log-MSE)=0.2579 | val_MAE(hours)=59.93\n",
            "\n",
            "========== Epoch 19/20 ==========\n",
            "  [Epoch 19 | Step 0/1162] AvgTrainLoss=0.2210\n",
            "  [Epoch 19 | Step 100/1162] AvgTrainLoss=0.1759\n",
            "  [Epoch 19 | Step 200/1162] AvgTrainLoss=0.1750\n",
            "  [Epoch 19 | Step 300/1162] AvgTrainLoss=0.1767\n",
            "  [Epoch 19 | Step 400/1162] AvgTrainLoss=0.1775\n",
            "  [Epoch 19 | Step 500/1162] AvgTrainLoss=0.1792\n",
            "  [Epoch 19 | Step 600/1162] AvgTrainLoss=0.1795\n",
            "  [Epoch 19 | Step 700/1162] AvgTrainLoss=0.1806\n",
            "  [Epoch 19 | Step 800/1162] AvgTrainLoss=0.1813\n",
            "  [Epoch 19 | Step 900/1162] AvgTrainLoss=0.1825\n",
            "  [Epoch 19 | Step 1000/1162] AvgTrainLoss=0.1834\n",
            "  [Epoch 19 | Step 1100/1162] AvgTrainLoss=0.1839\n",
            "[Epoch 019] train_loss(log-MSE)=0.1845 | val_loss(log-MSE)=0.2551 | val_MAE(hours)=59.09\n",
            "\n",
            "========== Epoch 20/20 ==========\n",
            "  [Epoch 20 | Step 0/1162] AvgTrainLoss=0.1516\n",
            "  [Epoch 20 | Step 100/1162] AvgTrainLoss=0.1720\n",
            "  [Epoch 20 | Step 200/1162] AvgTrainLoss=0.1725\n",
            "  [Epoch 20 | Step 300/1162] AvgTrainLoss=0.1736\n",
            "  [Epoch 20 | Step 400/1162] AvgTrainLoss=0.1751\n",
            "  [Epoch 20 | Step 500/1162] AvgTrainLoss=0.1755\n",
            "  [Epoch 20 | Step 600/1162] AvgTrainLoss=0.1764\n",
            "  [Epoch 20 | Step 700/1162] AvgTrainLoss=0.1774\n",
            "  [Epoch 20 | Step 800/1162] AvgTrainLoss=0.1782\n",
            "  [Epoch 20 | Step 900/1162] AvgTrainLoss=0.1791\n",
            "  [Epoch 20 | Step 1000/1162] AvgTrainLoss=0.1799\n",
            "  [Epoch 20 | Step 1100/1162] AvgTrainLoss=0.1804\n",
            "[Epoch 020] train_loss(log-MSE)=0.1808 | val_loss(log-MSE)=0.2585 | val_MAE(hours)=60.10\n",
            "Training finished. Best val_loss: 0.2518418602500408\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# 1. Model / Configuration Generation\n",
        "\n",
        "cfg = ModelConfig(\n",
        "    num_genders=len(gender_stoi),\n",
        "    num_races=len(race_stoi),\n",
        "    num_services=len(service_stoi),\n",
        "    num_drg_codes=len(drg_code_stoi),\n",
        "    diag_vocab_size=len(diag_stoi),\n",
        "    proc_vocab_size=len(proc_all_stoi),\n",
        "    med_vocab_size=len(med_stoi),\n",
        "    order_vocab_size=len(order_stoi),\n",
        ")\n",
        "\n",
        "model = MultiModalLOSModel(cfg)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()  # MSE based on log(1+LOS)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "NUM_EPOCHS = 20 \n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model_path = \"los_multibranch_attention_best.pt\" \n",
        "print(\"Using device:\", device)\n",
        "\n",
        "# 2. Training Loop\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # ---------- Train ----------\n",
        "    model.train()\n",
        "    train_loss_sum = 0.0\n",
        "    train_count = 0\n",
        "\n",
        "    print(f\"\\n========== Epoch {epoch}/{NUM_EPOCHS} ==========\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        # Move tensors to device\n",
        "        batch = {k: (v.to(device) if torch.is_tensor(v) else v)\n",
        "                 for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(\n",
        "            age=batch[\"age\"],\n",
        "            gender_idx=batch[\"gender_id\"],\n",
        "            race_idx=batch[\"race_id\"],\n",
        "            service_idx=batch[\"service_id\"],\n",
        "            drg_code_idx=batch[\"drg_code_id\"],\n",
        "            drg_severity=batch[\"drg_severity\"],\n",
        "            drg_mortality=batch[\"drg_mortality\"],\n",
        "            diag_codes=batch[\"diag_codes\"],\n",
        "            diag_offsets=batch[\"diag_offsets\"],\n",
        "            proc_codes=batch[\"proc_codes\"],\n",
        "            proc_offsets=batch[\"proc_offsets\"],\n",
        "            med_codes=batch[\"med_codes\"],\n",
        "            med_offsets=batch[\"med_offsets\"],\n",
        "            order_codes=batch[\"order_codes\"],\n",
        "            order_offsets=batch[\"order_offsets\"],\n",
        "        )\n",
        "\n",
        "        y_true = batch[\"target\"]  # log(1+LOS)\n",
        "        loss = criterion(y_pred, y_true)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y_true.size(0)\n",
        "        train_loss_sum += loss.item() * bs\n",
        "        train_count += bs\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            avg_loss = train_loss_sum / train_count\n",
        "            print(f\"  [Epoch {epoch} | Step {batch_idx}/{len(train_loader)}] \"\n",
        "                  f\"AvgTrainLoss={avg_loss:.4f}\")\n",
        "\n",
        "    train_loss = train_loss_sum / train_count\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    model.eval()\n",
        "    val_loss_sum = 0.0\n",
        "    val_count = 0\n",
        "    val_mae_hours_sum = 0.0  # MAE based on actual LOS (hours)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: (v.to(device) if torch.is_tensor(v) else v)\n",
        "                     for k, v in batch.items()}\n",
        "\n",
        "            y_pred = model(\n",
        "                age=batch[\"age\"],\n",
        "                gender_idx=batch[\"gender_id\"],\n",
        "                race_idx=batch[\"race_id\"],\n",
        "                service_idx=batch[\"service_id\"],\n",
        "                drg_code_idx=batch[\"drg_code_id\"],\n",
        "                drg_severity=batch[\"drg_severity\"],\n",
        "                drg_mortality=batch[\"drg_mortality\"],\n",
        "                diag_codes=batch[\"diag_codes\"],\n",
        "                diag_offsets=batch[\"diag_offsets\"],\n",
        "                proc_codes=batch[\"proc_codes\"],\n",
        "                proc_offsets=batch[\"proc_offsets\"],\n",
        "                med_codes=batch[\"med_codes\"],\n",
        "                med_offsets=batch[\"med_offsets\"],\n",
        "                order_codes=batch[\"order_codes\"],\n",
        "                order_offsets=batch[\"order_offsets\"],\n",
        "            )\n",
        "\n",
        "            y_true = batch[\"target\"]\n",
        "\n",
        "            loss = criterion(y_pred, y_true)\n",
        "\n",
        "            bs = y_true.size(0)\n",
        "            val_loss_sum += loss.item() * bs\n",
        "            val_count += bs\n",
        "\n",
        "            # Convert log(1+LOS) -> actual LOS (hours) and calculate MAE\n",
        "            y_true_hours = torch.expm1(y_true)\n",
        "            y_pred_hours = torch.expm1(y_pred)\n",
        "\n",
        "            mae_hours = torch.abs(y_pred_hours - y_true_hours).sum().item()\n",
        "            val_mae_hours_sum += mae_hours\n",
        "\n",
        "    val_loss = val_loss_sum / val_count\n",
        "    val_mae_hours = val_mae_hours_sum / val_count\n",
        "\n",
        "    print(f\"[Epoch {epoch:03d}] \"\n",
        "          f\"train_loss(log-MSE)={train_loss:.4f} | \"\n",
        "          f\"val_loss(log-MSE)={val_loss:.4f} | \"\n",
        "          f\"val_MAE(hours)={val_mae_hours:.2f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"  \\u21B3 Best model updated, saved to {best_model_path}\")\n",
        "\n",
        "print(\"Training finished. Best val_loss:\", best_val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vDQN05751XD",
        "outputId": "f4908f4c-4eea-43a3-9539-237fdac7e1ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Test set MAE =====\n",
            "Test MAE (hours): 57.73\n",
            "Test MAE (days) : 2.41\n"
          ]
        }
      ],
      "source": [
        "# 3. Calculate MAE (hours) on Test set\n",
        "\n",
        "best_model = MultiModalLOSModel(cfg).to(device)\n",
        "best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "test_abs_error_sum = 0.0\n",
        "test_count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: (v.to(device) if torch.is_tensor(v) else v)\n",
        "                 for k, v in batch.items()}\n",
        "\n",
        "        y_pred = best_model(\n",
        "            age=batch[\"age\"],\n",
        "            gender_idx=batch[\"gender_id\"],\n",
        "            race_idx=batch[\"race_id\"],\n",
        "            service_idx=batch[\"service_id\"],\n",
        "            drg_code_idx=batch[\"drg_code_id\"],\n",
        "            drg_severity=batch[\"drg_severity\"],\n",
        "            drg_mortality=batch[\"drg_mortality\"],\n",
        "            diag_codes=batch[\"diag_codes\"],\n",
        "            diag_offsets=batch[\"diag_offsets\"],\n",
        "            proc_codes=batch[\"proc_codes\"],\n",
        "            proc_offsets=batch[\"proc_offsets\"],\n",
        "            med_codes=batch[\"med_codes\"],\n",
        "            med_offsets=batch[\"med_offsets\"],\n",
        "            order_codes=batch[\"order_codes\"],\n",
        "            order_offsets=batch[\"order_offsets\"],\n",
        "        )\n",
        "\n",
        "        y_true = batch[\"target\"]  # log(1+LOS)\n",
        "\n",
        "        # Convert log(1+LOS) -> actual hours\n",
        "        y_true_hours = torch.expm1(y_true)\n",
        "        y_pred_hours = torch.expm1(y_pred)\n",
        "\n",
        "        abs_err = torch.abs(y_pred_hours - y_true_hours)\n",
        "        test_abs_error_sum += abs_err.sum().item()\n",
        "        test_count += y_true.size(0)\n",
        "\n",
        "test_mae_hours = test_abs_error_sum / test_count\n",
        "\n",
        "print(f\"\\n===== Test set MAE =====\")\n",
        "print(f\"Test MAE (hours): {test_mae_hours:.2f}\")\n",
        "print(f\"Test MAE (days) : {test_mae_hours / 24:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94c59090",
        "outputId": "1ff09f73-a656-45ab-fc4b-162b5f5ad4a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====== Average Attention Weights ======\n",
            "Average Tabular Attention Weight: 0.3788\n",
            "Average Diag Attention Weight: 0.2500\n",
            "Average Proc Attention Weight: 0.1966\n",
            "Average Med Attention Weight: 0.1420\n",
            "Average Order Attention Weight: 0.0326\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "best_model = MultiModalLOSModel(cfg).to(device)\n",
        "best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "all_attention_weights = {\n",
        "    \"tabular\": [],\n",
        "    \"diag\": [],\n",
        "    \"proc\": [],\n",
        "    \"med\": [],\n",
        "    \"order\": [],\n",
        "}\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = {k: (v.to(device) if torch.is_tensor(v) else v)\n",
        "                 for k, v in batch.items()}\n",
        "\n",
        "        # Tabular branch\n",
        "        g_emb = best_model.gender_emb(batch[\"gender_id\"])\n",
        "        r_emb = best_model.race_emb(batch[\"race_id\"])\n",
        "        s_emb = best_model.service_emb(batch[\"service_id\"])\n",
        "        d_emb = best_model.drg_emb(batch[\"drg_code_id\"])\n",
        "\n",
        "        age = batch[\"age\"].float().unsqueeze(-1)\n",
        "        sev = batch[\"drg_severity\"].float().unsqueeze(-1)\n",
        "        mort = batch[\"drg_mortality\"].float().unsqueeze(-1)\n",
        "\n",
        "        tabular_feat = torch.cat(\n",
        "            [age, sev, mort, g_emb, r_emb, s_emb, d_emb],\n",
        "            dim=-1\n",
        "        )\n",
        "        h_tab = best_model.tabular_mlp(tabular_feat)\n",
        "\n",
        "        # Diagnostic branch\n",
        "        diag_bag = best_model.diag_emb(batch[\"diag_codes\"], batch[\"diag_offsets\"])\n",
        "        h_diag = best_model.diag_mlp(diag_bag)\n",
        "\n",
        "        # Procedure branch\n",
        "        proc_bag = best_model.proc_emb(batch[\"proc_codes\"], batch[\"proc_offsets\"])\n",
        "        h_proc = best_model.proc_mlp(proc_bag)\n",
        "\n",
        "        # Medication branch\n",
        "        med_bag = best_model.med_emb(batch[\"med_codes\"], batch[\"med_offsets\"])\n",
        "        h_med = best_model.med_mlp(med_bag)\n",
        "\n",
        "        # Order-type branch\n",
        "        order_bag = best_model.order_emb(batch[\"order_codes\"], batch[\"order_offsets\"])\n",
        "        h_order = best_model.order_mlp(order_bag)\n",
        "\n",
        "        # Apply self-attention mechanism to get weights\n",
        "        h_pre_attention = torch.cat([h_tab, h_diag, h_proc, h_med, h_order], dim=-1)\n",
        "        alpha_raw = best_model.attention_mlp(h_pre_attention)\n",
        "        alpha_weights = F.softmax(alpha_raw, dim=-1)\n",
        "\n",
        "        # Split alpha_weights into individual branch weights and store\n",
        "        all_attention_weights[\"tabular\"].extend(alpha_weights[:, 0].cpu().numpy())\n",
        "        all_attention_weights[\"diag\"].extend(alpha_weights[:, 1].cpu().numpy())\n",
        "        all_attention_weights[\"proc\"].extend(alpha_weights[:, 2].cpu().numpy())\n",
        "        all_attention_weights[\"med\"].extend(alpha_weights[:, 3].cpu().numpy())\n",
        "        all_attention_weights[\"order\"].extend(alpha_weights[:, 4].cpu().numpy())\n",
        "\n",
        "# 4. Calculate and print the mean of all collected attention weights\n",
        "print(\"\\n====== Average Attention Weights ======\")\n",
        "for branch_name, weights in all_attention_weights.items():\n",
        "    avg_weight = np.mean(weights)\n",
        "    print(f\"Average {branch_name.capitalize()} Attention Weight: {avg_weight:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_toUdAloFRy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
