{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_V08e05Ftqyv",
        "outputId": "be8229f9-ab7a-41ca-dd6e-c302f9757791"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(424803, 17)\n",
            "Index(['subject_id', 'hadm_id', 'admittime', 'dischtime', 'race', 'los_hours',\n",
            "       'gender', 'anchor_age', 'curr_service', 'hcpcs_cd_list',\n",
            "       'diagnoses_icd_code_list', 'procedures_icd_code_list', 'drg_code',\n",
            "       'drg_severity', 'drg_mortality', 'medication_list', 'order_type_list'],\n",
            "      dtype='object')\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from collections import Counter\n",
        "import json\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_parquet(\"los_dataset_24h.parquet\")\n",
        "\n",
        "print(df.shape)\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTrTAv2ht3aZ"
      },
      "outputs": [],
      "source": [
        "def build_label_encoder(series):\n",
        "    classes = sorted(series.unique())\n",
        "    stoi = {c: i for i, c in enumerate(classes)}\n",
        "    return stoi\n",
        "\n",
        "\n",
        "def build_vocab_from_list_column(df, col, min_freq=1, add_unk=True):\n",
        "    counter = Counter()\n",
        "    for lst in df[col]:\n",
        "        counter.update(lst)\n",
        "\n",
        "    stoi = {}\n",
        "    idx = 0\n",
        "    if add_unk:\n",
        "        stoi[\"<UNK>\"] = idx\n",
        "        idx += 1\n",
        "\n",
        "    for token, freq in counter.items():\n",
        "        if freq >= min_freq:\n",
        "            if token not in stoi:\n",
        "                stoi[token] = idx\n",
        "                idx += 1\n",
        "\n",
        "    return stoi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2LpjEFBuIUx",
        "outputId": "efd028f6-3e9c-472c-b663-e49f049f1078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "num_genders: 2\n",
            "num_races: 33\n",
            "num_services: 21\n",
            "num_drg_codes: 301\n"
          ]
        }
      ],
      "source": [
        "# Encoders for gender, race, curr_service, drg_code\n",
        "gender_stoi = build_label_encoder(df[\"gender\"])\n",
        "race_stoi = build_label_encoder(df[\"race\"])\n",
        "service_stoi = build_label_encoder(df[\"curr_service\"])\n",
        "\n",
        "# drg_code is already an int, but treat it as a \"category\" and map it to an index\n",
        "drg_code_stoi = build_label_encoder(df[\"drg_code\"].astype(int))\n",
        "\n",
        "print(\"num_genders:\", len(gender_stoi))\n",
        "print(\"num_races:\", len(race_stoi))\n",
        "print(\"num_services:\", len(service_stoi))\n",
        "print(\"num_drg_codes:\", len(drg_code_stoi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jysvDQ6KuKI3",
        "outputId": "bc196f05-f3a2-467e-a5b8-bbd17112eb12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "diag vocab size: 27701\n",
            "proc vocab size: 12184\n",
            "hcpcs vocab size: 1925\n",
            "med vocab size: 3358\n",
            "order vocab size: 17\n"
          ]
        }
      ],
      "source": [
        "list_cols = [\n",
        "    \"diagnoses_icd_code_list\",\n",
        "    \"procedures_icd_code_list\",\n",
        "    \"hcpcs_cd_list\",\n",
        "    \"medication_list\",\n",
        "    \"order_type_list\",\n",
        "]\n",
        "\n",
        "diag_stoi = build_vocab_from_list_column(df, \"diagnoses_icd_code_list\", min_freq=1)\n",
        "proc_stoi = build_vocab_from_list_column(df, \"procedures_icd_code_list\", min_freq=1)\n",
        "hcpcs_stoi = build_vocab_from_list_column(df, \"hcpcs_cd_list\", min_freq=1)\n",
        "med_stoi = build_vocab_from_list_column(df, \"medication_list\", min_freq=1)\n",
        "order_stoi = build_vocab_from_list_column(df, \"order_type_list\", min_freq=1)\n",
        "\n",
        "print(\"diag vocab size:\", len(diag_stoi))\n",
        "print(\"proc vocab size:\", len(proc_stoi))\n",
        "print(\"hcpcs vocab size:\", len(hcpcs_stoi))\n",
        "print(\"med vocab size:\", len(med_stoi))\n",
        "print(\"order vocab size:\", len(order_stoi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prFr0KUjuNiy",
        "outputId": "1732fd28-02fb-4d37-8b8c-33d9ba25690f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "combined proc vocab size: 14108\n"
          ]
        }
      ],
      "source": [
        "# Combine proc_stoi and hcpcs_stoi into a single unified vocabulary\n",
        "proc_all_stoi = {}\n",
        "idx = 0\n",
        "\n",
        "# Only one UNK\n",
        "proc_all_stoi[\"<UNK>\"] = idx\n",
        "idx += 1\n",
        "\n",
        "# Procedures first\n",
        "for k in proc_stoi.keys():\n",
        "    if k == \"<UNK>\":\n",
        "        continue\n",
        "    proc_all_stoi[\"PROC_\" + k] = idx\n",
        "    idx += 1\n",
        "\n",
        "# Then hcpcs\n",
        "for k in hcpcs_stoi.keys():\n",
        "    if k == \"<UNK>\":\n",
        "        continue\n",
        "    key = \"HCPCS_\" + k\n",
        "    if key not in proc_all_stoi:\n",
        "        proc_all_stoi[key] = idx\n",
        "        idx += 1\n",
        "\n",
        "print(\"combined proc vocab size:\", len(proc_all_stoi))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vqz1gkzjuQpZ"
      },
      "outputs": [],
      "source": [
        "UNK_DIAG = diag_stoi[\"<UNK>\"]\n",
        "UNK_PROC = proc_all_stoi[\"<UNK>\"]\n",
        "UNK_MED = med_stoi[\"<UNK>\"]\n",
        "UNK_ORDER = order_stoi[\"<UNK>\"]\n",
        "\n",
        "def map_list_to_ids(lst, stoi, unk_token=\"<UNK>\"):\n",
        "    unk_idx = stoi.get(unk_token, None)\n",
        "    out = []\n",
        "    for x in lst:\n",
        "        idx = stoi.get(x)\n",
        "        if idx is None:\n",
        "            if unk_idx is not None:\n",
        "                out.append(unk_idx)\n",
        "        else:\n",
        "            out.append(idx)\n",
        "    return out\n",
        "\n",
        "# Diagnostic codes\n",
        "df[\"diag_ids\"] = df[\"diagnoses_icd_code_list\"].apply(\n",
        "    lambda lst: map_list_to_ids(lst, diag_stoi)\n",
        ")\n",
        "\n",
        "# Combined procedure + hcpcs IDs\n",
        "def build_proc_ids(row):\n",
        "    ids = []\n",
        "    for code in row[\"procedures_icd_code_list\"]:\n",
        "        tok = \"PROC_\" + code\n",
        "        ids.append(proc_all_stoi.get(tok, UNK_PROC))\n",
        "    for code in row[\"hcpcs_cd_list\"]:\n",
        "        tok = \"HCPCS_\" + code\n",
        "        ids.append(proc_all_stoi.get(tok, UNK_PROC))\n",
        "    return ids\n",
        "\n",
        "df[\"proc_ids\"] = df.apply(build_proc_ids, axis=1)\n",
        "\n",
        "# Medication\n",
        "df[\"med_ids\"] = df[\"medication_list\"].apply(\n",
        "    lambda lst: map_list_to_ids(lst, med_stoi)\n",
        ")\n",
        "\n",
        "# Order type\n",
        "df[\"order_ids\"] = df[\"order_type_list\"].apply(\n",
        "    lambda lst: map_list_to_ids(lst, order_stoi)\n",
        ")\n",
        "\n",
        "# Single categorical → ID\n",
        "df[\"gender_id\"] = df[\"gender\"].map(gender_stoi)\n",
        "df[\"race_id\"] = df[\"race\"].map(race_stoi)\n",
        "df[\"service_id\"] = df[\"curr_service\"].map(service_stoi)\n",
        "df[\"drg_code_id\"] = df[\"drg_code\"].astype(int).map(drg_code_stoi)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTxymZbB-_YY",
        "outputId": "1a5b5f1f-ca7c-4487-f5ff-1ca6d487b7a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================ SAMPLE DATA (after encoding) ================\n",
            "\n",
            "--- Row 0 ---\n",
            "gender                    : F\n",
            "race                      : WHITE\n",
            "curr_service              : MED\n",
            "drg_code                  : 279\n",
            "gender_id                 : 0\n",
            "race_id                   : 28\n",
            "service_id                : 7\n",
            "drg_code_id               : 135\n",
            "diagnoses_icd_code_list   : ['07071' '78959' '2875' '2761' '496' '5715' 'V08' '3051']\n",
            "diag_ids                  : [1, 2, 3, 4, 5, 6, 7, 8]\n",
            "procedures_icd_code_list  : ['5491']\n",
            "hcpcs_cd_list             : []\n",
            "proc_ids                  : [1]\n",
            "medication_list           : ['Raltegravir' 'Rifaximin' 'Sodium Chloride 0.9%  Flush'\n",
            " 'Calcium Carbonate' 'Rifaximin' 'Raltegravir'\n",
            " 'Emtricitabine-Tenofovir (Truvada)' 'Sulfameth/Trimethoprim DS'\n",
            " 'Furosemide' 'Tiotropium Bromide' 'Albuterol Inhaler' 'Lactulose'\n",
            " 'Heparin' 'Sodium Chloride 0.9%  Flush' 'Acetaminophen' 'Heparin'\n",
            " 'Lactulose' 'Albumin 25% (12.5g / 50mL)' 'Sodium Chloride 0.9%  Flush'\n",
            " 'Albumin 25% (12.5g / 50mL)' 'Albumin 25% (12.5g / 50mL)']\n",
            "med_ids                   : [1, 2, 3, 4, 2, 1, 5, 6, 7, 8, '...(+11 more)']\n",
            "order_type_list           : ['Medications' 'General Care' 'Nutrition' 'Blood Bank' 'Lab' 'Respiratory'\n",
            " 'Medications' 'Medications' 'ADT orders' 'Lab' 'Lab' 'Medications'\n",
            " 'ADT orders' 'ADT orders' 'ADT orders' 'ADT orders' 'Lab' 'ADT orders'\n",
            " 'Lab' 'General Care' 'Nutrition' 'Medications' 'ADT orders' 'Lab'\n",
            " 'IV therapy' 'Medications' 'General Care' 'General Care' 'Nutrition'\n",
            " 'Medications' 'Nutrition' 'Lab' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'Medications']\n",
            "order_ids                 : [1, 2, 3, 4, 5, 6, 1, 1, 7, 5, '...(+32 more)']\n",
            "\n",
            "\n",
            "--- Row 1 ---\n",
            "gender                    : F\n",
            "race                      : WHITE\n",
            "curr_service              : MED\n",
            "drg_code                  : 283\n",
            "gender_id                 : 0\n",
            "race_id                   : 28\n",
            "service_id                : 7\n",
            "drg_code_id               : 139\n",
            "diagnoses_icd_code_list   : ['07054' '78959' 'V462' '5715' '2767' '2761' '496' 'V08' '3051' '78791']\n",
            "diag_ids                  : [9, 2, 10, 6, 11, 4, 5, 7, 8, 12]\n",
            "procedures_icd_code_list  : ['5491']\n",
            "hcpcs_cd_list             : []\n",
            "proc_ids                  : [1]\n",
            "medication_list           : ['Heparin' 'Raltegravir' 'Rifaximin' 'Emtricitabine-Tenofovir (Truvada)'\n",
            " 'Lactulose' 'Fluticasone Propionate 110mcg' 'Tiotropium Bromide'\n",
            " 'Albuterol Inhaler' 'Calcium Gluconate' 'Dextrose 50%'\n",
            " 'Insulin (Regular) for Hyperkalemia' 'Sodium Polystyrene Sulfonate'\n",
            " 'TraMADOL (Ultram)' 'Lactulose' 'Heparin' 'Sodium Chloride 0.9%  Flush'\n",
            " 'Lactulose' 'Albuterol Inhaler' 'Insulin (Regular) for Hyperkalemia'\n",
            " 'Sodium Polystyrene Sulfonate' 'Calcium Gluconate' 'Dextrose 50%'\n",
            " 'Furosemide' 'Albumin 25% (12.5g / 50mL)' 'Calcium Carbonate'\n",
            " 'Raltegravir' 'Rifaximin' 'Zolpidem Tartrate'\n",
            " 'Fluticasone Propionate 110mcg' 'Albuterol Inhaler' 'Heparin'\n",
            " 'Sodium Chloride 0.9%  Flush' 'Sodium Chloride 0.9%  Flush'\n",
            " 'Calcium Carbonate']\n",
            "med_ids                   : [11, 1, 2, 5, 10, 14, 8, 9, 15, 16, '...(+24 more)']\n",
            "order_type_list           : ['Lab' 'ADT orders' 'Lab' 'General Care' 'General Care' 'Nutrition'\n",
            " 'Medications' 'ADT orders' 'Lab' 'IV therapy' 'Medications'\n",
            " 'General Care' 'General Care' 'General Care' 'Lab' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Radiology' 'Nutrition' 'Nutrition' 'ADT orders' 'Lab' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Cardiology' 'Lab'\n",
            " 'Medications' 'Consults' 'Consults' 'General Care' 'Medications' 'Lab'\n",
            " 'Medications' 'Lab' 'Lab' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'General Care' 'General Care' 'Medications' 'Medications'\n",
            " 'Lab']\n",
            "order_ids                 : [5, 7, 5, 2, 2, 3, 1, 7, 5, 8, '...(+45 more)']\n",
            "\n",
            "\n",
            "--- Row 2 ---\n",
            "gender                    : F\n",
            "race                      : WHITE\n",
            "curr_service              : MED\n",
            "drg_code                  : 207\n",
            "gender_id                 : 0\n",
            "race_id                   : 28\n",
            "service_id                : 7\n",
            "drg_code_id               : 103\n",
            "diagnoses_icd_code_list   : ['45829' '07044' '7994' '2761' '78959' '2767' '3051' 'V08' 'V4986' 'V462'\n",
            " '496' '29680' '5715']\n",
            "diag_ids                  : [13, 14, 15, 4, 2, 11, 8, 7, 16, 10, '...(+3 more)']\n",
            "procedures_icd_code_list  : []\n",
            "hcpcs_cd_list             : []\n",
            "proc_ids                  : []\n",
            "medication_list           : ['Lactulose' 'TraMADOL (Ultram)' 'Sodium Chloride 0.9%  Flush'\n",
            " 'Albumin 25% (12.5g / 50mL)' 'Bisacodyl' 'Calcium Carbonate'\n",
            " 'Docusate Sodium (Liquid)' 'Emtricitabine-Tenofovir (Truvada)'\n",
            " 'Fluticasone Propionate 110mcg' 'Heparin' 'Lactulose' 'Raltegravir'\n",
            " 'Rifaximin' 'Tiotropium Bromide']\n",
            "med_ids                   : [10, 19, 3, 13, 21, 4, 22, 5, 14, 11, '...(+4 more)']\n",
            "order_type_list           : ['Lab' 'ADT orders' 'Lab' 'General Care' 'General Care' 'General Care'\n",
            " 'Nutrition' 'Medications' 'ADT orders' 'Lab' 'General Care'\n",
            " 'General Care' 'General Care' 'Nutrition' 'Medications' 'Medications'\n",
            " 'Medications' 'General Care' 'Medications' 'Respiratory' 'General Care'\n",
            " 'Lab' 'Medications' 'Lab' 'Nutrition' 'Medications' 'Lab' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'Lab' 'ADT orders' 'Lab' 'Medications' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications' 'Medications' 'Medications' 'Medications' 'Medications'\n",
            " 'Nutrition' 'Nutrition' 'IV therapy' 'Medications' 'General Care'\n",
            " 'General Care' 'General Care' 'Lab' 'General Care' 'Medications'\n",
            " 'Medications' 'Lab' 'General Care' 'ADT orders' 'Lab' 'Medications'\n",
            " 'Consults' 'Consults' 'Medications' 'Medications' 'Medications'\n",
            " 'Medications']\n",
            "order_ids                 : [5, 7, 5, 2, 2, 2, 3, 1, 7, 5, '...(+61 more)']\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def summarize_list(lst, max_len=10):\n",
        "    \"\"\"\n",
        "    Summarizes a list by truncating it if it's too long.\n",
        "    \"\"\"\n",
        "    if lst is None:\n",
        "        return None\n",
        "    if len(lst) <= max_len:\n",
        "        return lst\n",
        "    return lst[:max_len] + [\"...(+{} more)\".format(len(lst) - max_len)]\n",
        "\n",
        "\n",
        "cols_to_show = [\n",
        "    \"gender\", \"race\", \"curr_service\", \"drg_code\",\n",
        "    \"gender_id\", \"race_id\", \"service_id\", \"drg_code_id\",\n",
        "    \"diagnoses_icd_code_list\", \"diag_ids\",\n",
        "    \"procedures_icd_code_list\", \"hcpcs_cd_list\", \"proc_ids\",\n",
        "    \"medication_list\", \"med_ids\",\n",
        "    \"order_type_list\", \"order_ids\",\n",
        "]\n",
        "\n",
        "print(\"\\n================ SAMPLE DATA (after encoding) ================\\n\")\n",
        "\n",
        "for i in range(3):\n",
        "    row = df.iloc[i]\n",
        "    print(f\"--- Row {i} ---\")\n",
        "\n",
        "    for c in cols_to_show:\n",
        "        val = row[c]\n",
        "\n",
        "        # Summarize list\n",
        "        if isinstance(val, list):\n",
        "            val = summarize_list(val)\n",
        "\n",
        "        print(f\"{c:25} : {val}\")\n",
        "\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "D0LX9ClZuTkx"
      },
      "outputs": [],
      "source": [
        "class LOSDataset(Dataset):\n",
        "    def __init__(self, df, use_log_target=True):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.use_log_target = use_log_target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        sample = {\n",
        "            # Tabular features\n",
        "            \"age\": float(row[\"anchor_age\"]),\n",
        "            \"gender_id\": int(row[\"gender_id\"]),\n",
        "            \"race_id\": int(row[\"race_id\"]),\n",
        "            \"service_id\": int(row[\"service_id\"]),\n",
        "            \"drg_code_id\": int(row[\"drg_code_id\"]),\n",
        "            \"drg_severity\": float(row[\"drg_severity\"]),\n",
        "            \"drg_mortality\": float(row[\"drg_mortality\"]),\n",
        "\n",
        "            # List-type IDs\n",
        "            \"diag_ids\": row[\"diag_ids\"],\n",
        "            \"proc_ids\": row[\"proc_ids\"],\n",
        "            \"med_ids\": row[\"med_ids\"],\n",
        "            \"order_ids\": row[\"order_ids\"],\n",
        "        }\n",
        "\n",
        "        # Target\n",
        "        los = float(row[\"los_hours\"])\n",
        "        if self.use_log_target:\n",
        "            sample[\"target\"] = np.log1p(los)\n",
        "        else:\n",
        "            sample[\"target\"] = los\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZBngU2IuZuA"
      },
      "outputs": [],
      "source": [
        "def los_collate_fn(batch):\n",
        "    B = len(batch)\n",
        "\n",
        "    # ----- Tabular Stack -----\n",
        "    age = torch.tensor([b[\"age\"] for b in batch], dtype=torch.float32)\n",
        "    gender_id = torch.tensor([b[\"gender_id\"] for b in batch], dtype=torch.long)\n",
        "    race_id = torch.tensor([b[\"race_id\"] for b in batch], dtype=torch.long)\n",
        "    service_id = torch.tensor([b[\"service_id\"] for b in batch], dtype=torch.long)\n",
        "    drg_code_id = torch.tensor([b[\"drg_code_id\"] for b in batch], dtype=torch.long)\n",
        "    drg_severity = torch.tensor([b[\"drg_severity\"] for b in batch], dtype=torch.float32)\n",
        "    drg_mortality = torch.tensor([b[\"drg_mortality\"] for b in batch], dtype=torch.float32)\n",
        "\n",
        "    target = torch.tensor([b[\"target\"] for b in batch], dtype=torch.float32)\n",
        "\n",
        "    # ----- List-type: diag / proc / med / order -----\n",
        "    def build_bag_inputs(key):\n",
        "        codes_all = []\n",
        "        offsets = [0]\n",
        "        for b in batch:\n",
        "            ids = b[key]\n",
        "            codes_all.extend(ids)\n",
        "            offsets.append(len(codes_all))\n",
        "        if len(codes_all) == 0:\n",
        "            # Handle cases where the list is empty for all admissions\n",
        "            codes_tensor = torch.empty(0, dtype=torch.long)\n",
        "        else:\n",
        "            codes_tensor = torch.tensor(codes_all, dtype=torch.long)\n",
        "        offsets_tensor = torch.tensor(offsets, dtype=torch.long)\n",
        "        return codes_tensor, offsets_tensor\n",
        "\n",
        "    diag_codes, diag_offsets = build_bag_inputs(\"diag_ids\")\n",
        "    proc_codes, proc_offsets = build_bag_inputs(\"proc_ids\")\n",
        "    med_codes, med_offsets = build_bag_inputs(\"med_ids\")\n",
        "    order_codes, order_offsets = build_bag_inputs(\"order_ids\")\n",
        "\n",
        "    batch_out = {\n",
        "        \"age\": age,\n",
        "        \"gender_id\": gender_id,\n",
        "        \"race_id\": race_id,\n",
        "        \"service_id\": service_id,\n",
        "        \"drg_code_id\": drg_code_id,\n",
        "        \"drg_severity\": drg_severity,\n",
        "        \"drg_mortality\": drg_mortality,\n",
        "        \"diag_codes\": diag_codes,\n",
        "        \"diag_offsets\": diag_offsets,\n",
        "        \"proc_codes\": proc_codes,\n",
        "        \"proc_offsets\": proc_offsets,\n",
        "        \"med_codes\": med_codes,\n",
        "        \"med_offsets\": med_offsets,\n",
        "        \"order_codes\": order_codes,\n",
        "        \"order_offsets\": order_offsets,\n",
        "        \"target\": target,\n",
        "    }\n",
        "\n",
        "    return batch_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lmEgyvbPuaOO",
        "outputId": "a9e04f42-b86f-4d5c-9e67-0e5fd111a28d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#train: 297362, #val: 63720, #test: 63721\n"
          ]
        }
      ],
      "source": [
        "from torch.utils.data import random_split, DataLoader\n",
        "import torch\n",
        "\n",
        "# Create Dataset\n",
        "dataset = LOSDataset(df, use_log_target=True)\n",
        "\n",
        "n_total = len(dataset)\n",
        "n_train = int(n_total * 0.7)\n",
        "n_val = int(n_total * 0.15)\n",
        "n_test = n_total - n_train - n_val   # Remaining\n",
        "\n",
        "g = torch.Generator().manual_seed(42)\n",
        "train_ds, val_ds, test_ds = random_split(dataset, [n_train, n_val, n_test], generator=g)\n",
        "\n",
        "train_loader = DataLoader(\n",
        "    train_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=True,\n",
        "    collate_fn=los_collate_fn,\n",
        ")\n",
        "\n",
        "val_loader = DataLoader(\n",
        "    val_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    collate_fn=los_collate_fn,\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=256,\n",
        "    shuffle=False,\n",
        "    collate_fn=los_collate_fn,\n",
        ")\n",
        "\n",
        "print(f\"#train: {len(train_ds)}, #val: {len(val_ds)}, #test: {len(test_ds)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SoKFkMU2udG8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from dataclasses import dataclass\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class ModelConfig:\n",
        "    # ----- Tabular categorical vocab sizes -----\n",
        "    num_genders: int          # e.g. {\"M\", \"F\"} -> 2\n",
        "    num_races: int            # unique race categories\n",
        "    num_services: int         # curr_service (MED, ORTHO, ...)\n",
        "    num_drg_codes: int        # drg_code vocab size\n",
        "\n",
        "    # ----- Code vocab sizes -----\n",
        "    diag_vocab_size: int      # diagnoses_icd_code_list vocab\n",
        "    proc_vocab_size: int      # Combined vocab for procedures_icd_code_list + hcpcs_cd_list\n",
        "    med_vocab_size: int       # medication_list vocab\n",
        "    order_vocab_size: int     # order_type_list vocab\n",
        "\n",
        "    # ----- Embedding dimensions (ADDED THESE) -----\n",
        "    emb_dim_gender: int = 4\n",
        "    emb_dim_race: int = 8\n",
        "    emb_dim_service: int = 8\n",
        "    emb_dim_drg: int = 16\n",
        "\n",
        "    emb_dim_diag: int = 32\n",
        "    emb_dim_proc: int = 32\n",
        "    emb_dim_med: int = 32\n",
        "    emb_dim_order: int = 16\n",
        "\n",
        "    # ----- Transformer parameters -----\n",
        "    transformer_d_model: int = 64 # Embedding dimension for transformer, output dim of individual branch MLPs\n",
        "    transformer_n_heads: int = 8  # Number of attention heads\n",
        "    transformer_n_layers: int = 2 # Number of transformer encoder layers\n",
        "\n",
        "    # ----- Hidden dimensions -----\n",
        "    tabular_hidden_dim: int = transformer_d_model\n",
        "    diag_hidden_dim: int = transformer_d_model\n",
        "    proc_hidden_dim: int = transformer_d_model\n",
        "    med_hidden_dim: int = transformer_d_model\n",
        "    order_hidden_dim: int = transformer_d_model\n",
        "\n",
        "    fusion_hidden_dim: int = 128\n",
        "    dropout: float = 0.2\n",
        "\n",
        "\n",
        "class MultiModalLOSModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-branch model for LOS prediction.\n",
        "\n",
        "    Branches:\n",
        "      - Tabular branch: age, gender, race, drg, severity, mortality, curr_service\n",
        "      - Diagnostic branch: diagnoses_icd_code_list\n",
        "      - Procedure branch: procedures_icd_code_list + hcpcs_cd_list\n",
        "      - Medication branch: medication_list\n",
        "      - Order-type branch: order_type_list\n",
        "\n",
        "    Fusion:\n",
        "      concat([h_tab, h_diag, h_proc, h_med, h_order]) -> transformer encdoer -> Linear layer -> scalar LOS prediction\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cfg: ModelConfig):\n",
        "        super().__init__()\n",
        "        self.cfg = cfg\n",
        "\n",
        "        # ------------- Tabular embeddings -------------\n",
        "        self.gender_emb = nn.Embedding(cfg.num_genders, cfg.emb_dim_gender)\n",
        "        self.race_emb = nn.Embedding(cfg.num_races, cfg.emb_dim_race)\n",
        "        self.service_emb = nn.Embedding(cfg.num_services, cfg.emb_dim_service)\n",
        "        self.drg_emb = nn.Embedding(cfg.num_drg_codes, cfg.emb_dim_drg)\n",
        "\n",
        "        # Tabular MLP input dim:\n",
        "        #   age(1) + severity(1) + mortality(1)\n",
        "        # + gender_emb + race_emb + service_emb + drg_emb\n",
        "        tab_in_dim = (\n",
        "            3\n",
        "            + cfg.emb_dim_gender\n",
        "            + cfg.emb_dim_race\n",
        "            + cfg.emb_dim_service\n",
        "            + cfg.emb_dim_drg\n",
        "        )\n",
        "\n",
        "        self.tabular_mlp = nn.Sequential(\n",
        "            nn.Linear(tab_in_dim, cfg.tabular_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "            nn.Linear(cfg.tabular_hidden_dim, cfg.tabular_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        # ------------- Code embeddings (EmbeddingBag: bag-of-codes) -------------\n",
        "        self.diag_emb = nn.EmbeddingBag(\n",
        "            cfg.diag_vocab_size, cfg.emb_dim_diag,\n",
        "            mode=\"mean\", include_last_offset=True\n",
        "        )\n",
        "        self.proc_emb = nn.EmbeddingBag(\n",
        "            cfg.proc_vocab_size, cfg.emb_dim_proc,\n",
        "            mode=\"mean\", include_last_offset=True\n",
        "        )\n",
        "        self.med_emb = nn.EmbeddingBag(\n",
        "            cfg.med_vocab_size, cfg.emb_dim_med,\n",
        "            mode=\"mean\", include_last_offset=True\n",
        "        )\n",
        "        self.order_emb = nn.EmbeddingBag(\n",
        "            cfg.order_vocab_size, cfg.emb_dim_order,\n",
        "            mode=\"mean\", include_last_offset=True\n",
        "        )\n",
        "\n",
        "        # Small projection MLP for each branch\n",
        "        self.diag_mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.emb_dim_diag, cfg.diag_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "        )\n",
        "        self.proc_mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.emb_dim_proc, cfg.proc_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "        )\n",
        "        self.med_mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.emb_dim_med, cfg.med_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "        )\n",
        "        self.order_mlp = nn.Sequential(\n",
        "            nn.Linear(cfg.emb_dim_order, cfg.order_hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(cfg.dropout),\n",
        "        )\n",
        "\n",
        "        # Positional/Type embeddings for modalities\n",
        "        # 5 modalities: Tabular, Diagnosis, Procedure, Medication, Order-Type\n",
        "        self.modality_pos_emb = nn.Embedding(5, cfg.transformer_d_model)\n",
        "\n",
        "        # ------------- Fusion: Transformer Encoder -------------\n",
        "        # The individual branch MLPs now output cfg.transformer_d_model dimensions.\n",
        "        # These outputs will form the input sequence to the Transformer Encoder.\n",
        "\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=cfg.transformer_d_model,\n",
        "            nhead=cfg.transformer_n_heads,\n",
        "            dropout=cfg.dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "\n",
        "        # Transformer Encoder definition\n",
        "        self.transformer_encoder = nn.TransformerEncoder(\n",
        "            encoder_layer,\n",
        "            num_layers=cfg.transformer_n_layers\n",
        "        )\n",
        "\n",
        "        self.out = nn.Linear(cfg.transformer_d_model, 1)\n",
        "\n",
        "    # ------------------------------------------------------------------\n",
        "    # Forward: batch input format\n",
        "    # ------------------------------------------------------------------\n",
        "    def forward(\n",
        "        self,\n",
        "        # ----- Tabular -----\n",
        "        age,               # (B,) float tensor (anchor_age or normalized)\n",
        "        gender_idx,        # (B,) long tensor\n",
        "        race_idx,          # (B,) long tensor\n",
        "        service_idx,       # (B,) long tensor (curr_service)\n",
        "        drg_code_idx,      # (B,) long tensor\n",
        "        drg_severity,      # (B,) float or long (recommend normalizing to float)\n",
        "        drg_mortality,     # (B,) float or long\n",
        "\n",
        "        # ----- Diagnoses (EmbeddingBag) -----\n",
        "        diag_codes,        # (N_diag_codes,) long tensor (flattened)\n",
        "        diag_offsets,      # (B+1,) long tensor, EmbeddingBag offset\n",
        "\n",
        "        # ----- Procedures (EmbeddingBag) -----\n",
        "        proc_codes,        # (N_proc_codes,) long tensor\n",
        "        proc_offsets,      # (B+1,) long tensor\n",
        "\n",
        "        # ----- Medications (EmbeddingBag) -----\n",
        "        med_codes,         # (N_med_codes,) long tensor\n",
        "        med_offsets,       # (B+1,) long tensor\n",
        "\n",
        "        # ----- Order types (EmbeddingBag) -----\n",
        "        order_codes,       # (N_order_codes,) long tensor\n",
        "        order_offsets,     # (B+1,) long tensor\n",
        "    ):\n",
        "        # --------- 1. Tabular branch ---------\n",
        "        # Embeddings\n",
        "        g_emb = self.gender_emb(gender_idx)   # (B, emb_dim_gender)\n",
        "        r_emb = self.race_emb(race_idx)       # (B, emb_dim_race)\n",
        "        s_emb = self.service_emb(service_idx) # (B, emb_dim_service)\n",
        "        d_emb = self.drg_emb(drg_code_idx)    # (B, emb_dim_drg)\n",
        "\n",
        "        # Cast continuous features to float\n",
        "        age = age.float().unsqueeze(-1)                 # (B, 1)\n",
        "        sev = drg_severity.float().unsqueeze(-1)        # (B, 1)\n",
        "        mort = drg_mortality.float().unsqueeze(-1)      # (B, 1)\n",
        "\n",
        "        tabular_feat = torch.cat(\n",
        "            [age, sev, mort, g_emb, r_emb, s_emb, d_emb],\n",
        "            dim=-1\n",
        "        )  # (B, tab_in_dim)\n",
        "\n",
        "        h_tab = self.tabular_mlp(tabular_feat)  # (B, tabular_hidden_dim)\n",
        "\n",
        "        # --------- 2. Diagnostic branch ---------\n",
        "        # EmbeddingBag: diag_codes, diag_offsets\n",
        "        # If include_last_offset=True, offsets length is B+1\n",
        "        diag_bag = self.diag_emb(diag_codes, diag_offsets)  # (B, emb_dim_diag)\n",
        "        h_diag = self.diag_mlp(diag_bag)  # (B, diag_hidden_dim)\n",
        "\n",
        "        # --------- 3. Procedure branch ---------\n",
        "        proc_bag = self.proc_emb(proc_codes, proc_offsets)  # (B, emb_dim_proc)\n",
        "        h_proc = self.proc_mlp(proc_bag)  # (B, proc_hidden_dim)\n",
        "\n",
        "        # --------- 4. Medication branch ---------\n",
        "        med_bag = self.med_emb(med_codes, med_offsets)  # (B, emb_dim_med)\n",
        "        h_med = self.med_mlp(med_bag)  # (B, med_hidden_dim)\n",
        "\n",
        "        # --------- 5. Order-type branch ---------\n",
        "        order_bag = self.order_emb(order_codes, order_offsets)  # (B, emb_dim_order)\n",
        "        h_order = self.order_mlp(order_bag)  # (B, order_hidden_dim)\n",
        "\n",
        "        # --------- 6. Fusion with Transformer ---------\n",
        "        # Each branch's output is (B, transformer_d_model)\n",
        "        # Stack them to form a sequence for the Transformer: (B, num_branches, transformer_d_model)\n",
        "        h = torch.stack([h_tab, h_diag, h_proc, h_med, h_order], dim=1)\n",
        "\n",
        "        # Add modality positional embeddings\n",
        "        modality_indices = torch.arange(5, device=h.device).unsqueeze(0) # (1, 5)\n",
        "        modality_pos_embs = self.modality_pos_emb(modality_indices) # (1, 5, transformer_d_model)\n",
        "        h = h + modality_pos_embs # (B, 5, transformer_d_model) + (1, 5, transformer_d_model)\n",
        "\n",
        "        # Pass through Transformer Encoder\n",
        "        h_transformer_output = self.transformer_encoder(h) # (B, num_branches, transformer_d_model)\n",
        "\n",
        "        # For regression, average the output sequence or take the representation of a specific token.\n",
        "        h_fused = h_transformer_output.mean(dim=1) # (B, transformer_d_model)\n",
        "\n",
        "        out = self.out(h_fused).squeeze(-1)  # (B,)\n",
        "\n",
        "        # out = predicted LOS (can be directly hours, or designed to predict log1p)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZm2f-aivdkb",
        "outputId": "55dd5940-2b21-44cb-8b6d-456cc401647c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "\n",
            "========== Epoch 1/20 ==========\n",
            "  [Epoch 1 | Step 0/1162] AvgTrainLoss=23.3204\n",
            "  [Epoch 1 | Step 100/1162] AvgTrainLoss=0.9138\n",
            "  [Epoch 1 | Step 200/1162] AvgTrainLoss=0.7835\n",
            "  [Epoch 1 | Step 300/1162] AvgTrainLoss=0.7166\n",
            "  [Epoch 1 | Step 400/1162] AvgTrainLoss=0.6508\n",
            "  [Epoch 1 | Step 500/1162] AvgTrainLoss=0.6036\n",
            "  [Epoch 1 | Step 600/1162] AvgTrainLoss=0.5682\n",
            "  [Epoch 1 | Step 700/1162] AvgTrainLoss=0.5392\n",
            "  [Epoch 1 | Step 800/1162] AvgTrainLoss=0.5152\n",
            "  [Epoch 1 | Step 900/1162] AvgTrainLoss=0.4958\n",
            "  [Epoch 1 | Step 1000/1162] AvgTrainLoss=0.4787\n",
            "  [Epoch 1 | Step 1100/1162] AvgTrainLoss=0.4643\n",
            "[Epoch 001] train_loss(log-MSE)=0.4565 | val_loss(log-MSE)=0.3226 | val_MAE(hours)=66.57\n",
            "  → Best model updated, saved to los_multibranch_best.pt\n",
            "\n",
            "========== Epoch 2/20 ==========\n",
            "  [Epoch 2 | Step 0/1162] AvgTrainLoss=0.2899\n",
            "  [Epoch 2 | Step 100/1162] AvgTrainLoss=0.3036\n",
            "  [Epoch 2 | Step 200/1162] AvgTrainLoss=0.3060\n",
            "  [Epoch 2 | Step 300/1162] AvgTrainLoss=0.3016\n",
            "  [Epoch 2 | Step 400/1162] AvgTrainLoss=0.2995\n",
            "  [Epoch 2 | Step 500/1162] AvgTrainLoss=0.2975\n",
            "  [Epoch 2 | Step 600/1162] AvgTrainLoss=0.2969\n",
            "  [Epoch 2 | Step 700/1162] AvgTrainLoss=0.2958\n",
            "  [Epoch 2 | Step 800/1162] AvgTrainLoss=0.2943\n",
            "  [Epoch 2 | Step 900/1162] AvgTrainLoss=0.2936\n",
            "  [Epoch 2 | Step 1000/1162] AvgTrainLoss=0.2923\n",
            "  [Epoch 2 | Step 1100/1162] AvgTrainLoss=0.2914\n",
            "[Epoch 002] train_loss(log-MSE)=0.2908 | val_loss(log-MSE)=0.3086 | val_MAE(hours)=66.82\n",
            "  → Best model updated, saved to los_multibranch_best.pt\n",
            "\n",
            "========== Epoch 3/20 ==========\n",
            "  [Epoch 3 | Step 0/1162] AvgTrainLoss=0.2305\n",
            "  [Epoch 3 | Step 100/1162] AvgTrainLoss=0.2763\n",
            "  [Epoch 3 | Step 200/1162] AvgTrainLoss=0.2728\n",
            "  [Epoch 3 | Step 300/1162] AvgTrainLoss=0.2714\n",
            "  [Epoch 3 | Step 400/1162] AvgTrainLoss=0.2703\n",
            "  [Epoch 3 | Step 500/1162] AvgTrainLoss=0.2714\n",
            "  [Epoch 3 | Step 600/1162] AvgTrainLoss=0.2713\n",
            "  [Epoch 3 | Step 700/1162] AvgTrainLoss=0.2713\n",
            "  [Epoch 3 | Step 800/1162] AvgTrainLoss=0.2715\n",
            "  [Epoch 3 | Step 900/1162] AvgTrainLoss=0.2710\n",
            "  [Epoch 3 | Step 1000/1162] AvgTrainLoss=0.2708\n",
            "  [Epoch 3 | Step 1100/1162] AvgTrainLoss=0.2701\n",
            "[Epoch 003] train_loss(log-MSE)=0.2704 | val_loss(log-MSE)=0.2723 | val_MAE(hours)=62.42\n",
            "  → Best model updated, saved to los_multibranch_best.pt\n",
            "\n",
            "========== Epoch 4/20 ==========\n",
            "  [Epoch 4 | Step 0/1162] AvgTrainLoss=0.2976\n",
            "  [Epoch 4 | Step 100/1162] AvgTrainLoss=0.2561\n",
            "  [Epoch 4 | Step 200/1162] AvgTrainLoss=0.2554\n",
            "  [Epoch 4 | Step 300/1162] AvgTrainLoss=0.2542\n",
            "  [Epoch 4 | Step 400/1162] AvgTrainLoss=0.2550\n",
            "  [Epoch 4 | Step 500/1162] AvgTrainLoss=0.2558\n",
            "  [Epoch 4 | Step 600/1162] AvgTrainLoss=0.2573\n",
            "  [Epoch 4 | Step 700/1162] AvgTrainLoss=0.2583\n",
            "  [Epoch 4 | Step 800/1162] AvgTrainLoss=0.2576\n",
            "  [Epoch 4 | Step 900/1162] AvgTrainLoss=0.2578\n",
            "  [Epoch 4 | Step 1000/1162] AvgTrainLoss=0.2581\n",
            "  [Epoch 4 | Step 1100/1162] AvgTrainLoss=0.2584\n",
            "[Epoch 004] train_loss(log-MSE)=0.2586 | val_loss(log-MSE)=0.2730 | val_MAE(hours)=61.58\n",
            "\n",
            "========== Epoch 5/20 ==========\n",
            "  [Epoch 5 | Step 0/1162] AvgTrainLoss=0.2852\n",
            "  [Epoch 5 | Step 100/1162] AvgTrainLoss=0.2450\n",
            "  [Epoch 5 | Step 200/1162] AvgTrainLoss=0.2482\n",
            "  [Epoch 5 | Step 300/1162] AvgTrainLoss=0.2486\n",
            "  [Epoch 5 | Step 400/1162] AvgTrainLoss=0.2476\n",
            "  [Epoch 5 | Step 500/1162] AvgTrainLoss=0.2483\n",
            "  [Epoch 5 | Step 600/1162] AvgTrainLoss=0.2490\n",
            "  [Epoch 5 | Step 700/1162] AvgTrainLoss=0.2491\n",
            "  [Epoch 5 | Step 800/1162] AvgTrainLoss=0.2498\n",
            "  [Epoch 5 | Step 900/1162] AvgTrainLoss=0.2502\n",
            "  [Epoch 5 | Step 1000/1162] AvgTrainLoss=0.2502\n",
            "  [Epoch 5 | Step 1100/1162] AvgTrainLoss=0.2508\n",
            "[Epoch 005] train_loss(log-MSE)=0.2506 | val_loss(log-MSE)=0.2836 | val_MAE(hours)=62.62\n",
            "\n",
            "========== Epoch 6/20 ==========\n",
            "  [Epoch 6 | Step 0/1162] AvgTrainLoss=0.2388\n",
            "  [Epoch 6 | Step 100/1162] AvgTrainLoss=0.2386\n",
            "  [Epoch 6 | Step 200/1162] AvgTrainLoss=0.2411\n",
            "  [Epoch 6 | Step 300/1162] AvgTrainLoss=0.2414\n",
            "  [Epoch 6 | Step 400/1162] AvgTrainLoss=0.2410\n",
            "  [Epoch 6 | Step 500/1162] AvgTrainLoss=0.2419\n",
            "  [Epoch 6 | Step 600/1162] AvgTrainLoss=0.2435\n",
            "  [Epoch 6 | Step 700/1162] AvgTrainLoss=0.2439\n",
            "  [Epoch 6 | Step 800/1162] AvgTrainLoss=0.2441\n",
            "  [Epoch 6 | Step 900/1162] AvgTrainLoss=0.2439\n",
            "  [Epoch 6 | Step 1000/1162] AvgTrainLoss=0.2442\n",
            "  [Epoch 6 | Step 1100/1162] AvgTrainLoss=0.2443\n",
            "[Epoch 006] train_loss(log-MSE)=0.2442 | val_loss(log-MSE)=0.3227 | val_MAE(hours)=70.98\n",
            "\n",
            "========== Epoch 7/20 ==========\n",
            "  [Epoch 7 | Step 0/1162] AvgTrainLoss=0.2593\n",
            "  [Epoch 7 | Step 100/1162] AvgTrainLoss=0.2339\n",
            "  [Epoch 7 | Step 200/1162] AvgTrainLoss=0.2332\n",
            "  [Epoch 7 | Step 300/1162] AvgTrainLoss=0.2334\n",
            "  [Epoch 7 | Step 400/1162] AvgTrainLoss=0.2334\n",
            "  [Epoch 7 | Step 500/1162] AvgTrainLoss=0.2348\n",
            "  [Epoch 7 | Step 600/1162] AvgTrainLoss=0.2356\n",
            "  [Epoch 7 | Step 700/1162] AvgTrainLoss=0.2366\n",
            "  [Epoch 7 | Step 800/1162] AvgTrainLoss=0.2376\n",
            "  [Epoch 7 | Step 900/1162] AvgTrainLoss=0.2380\n",
            "  [Epoch 7 | Step 1000/1162] AvgTrainLoss=0.2393\n",
            "  [Epoch 7 | Step 1100/1162] AvgTrainLoss=0.2395\n",
            "[Epoch 007] train_loss(log-MSE)=0.2395 | val_loss(log-MSE)=0.3414 | val_MAE(hours)=72.36\n",
            "\n",
            "========== Epoch 8/20 ==========\n",
            "  [Epoch 8 | Step 0/1162] AvgTrainLoss=0.2648\n",
            "  [Epoch 8 | Step 100/1162] AvgTrainLoss=0.2241\n",
            "  [Epoch 8 | Step 200/1162] AvgTrainLoss=0.2253\n",
            "  [Epoch 8 | Step 300/1162] AvgTrainLoss=0.2265\n",
            "  [Epoch 8 | Step 400/1162] AvgTrainLoss=0.2281\n",
            "  [Epoch 8 | Step 500/1162] AvgTrainLoss=0.2287\n",
            "  [Epoch 8 | Step 600/1162] AvgTrainLoss=0.2299\n",
            "  [Epoch 8 | Step 700/1162] AvgTrainLoss=0.2304\n",
            "  [Epoch 8 | Step 800/1162] AvgTrainLoss=0.2307\n",
            "  [Epoch 8 | Step 900/1162] AvgTrainLoss=0.2317\n",
            "  [Epoch 8 | Step 1000/1162] AvgTrainLoss=0.2327\n",
            "  [Epoch 8 | Step 1100/1162] AvgTrainLoss=0.2333\n",
            "[Epoch 008] train_loss(log-MSE)=0.2336 | val_loss(log-MSE)=0.2774 | val_MAE(hours)=62.30\n",
            "\n",
            "========== Epoch 9/20 ==========\n",
            "  [Epoch 9 | Step 0/1162] AvgTrainLoss=0.2176\n",
            "  [Epoch 9 | Step 100/1162] AvgTrainLoss=0.2276\n",
            "  [Epoch 9 | Step 200/1162] AvgTrainLoss=0.2267\n",
            "  [Epoch 9 | Step 300/1162] AvgTrainLoss=0.2253\n",
            "  [Epoch 9 | Step 400/1162] AvgTrainLoss=0.2263\n",
            "  [Epoch 9 | Step 500/1162] AvgTrainLoss=0.2272\n",
            "  [Epoch 9 | Step 600/1162] AvgTrainLoss=0.2279\n",
            "  [Epoch 9 | Step 700/1162] AvgTrainLoss=0.2279\n",
            "  [Epoch 9 | Step 800/1162] AvgTrainLoss=0.2280\n",
            "  [Epoch 9 | Step 900/1162] AvgTrainLoss=0.2284\n",
            "  [Epoch 9 | Step 1000/1162] AvgTrainLoss=0.2287\n",
            "  [Epoch 9 | Step 1100/1162] AvgTrainLoss=0.2293\n",
            "[Epoch 009] train_loss(log-MSE)=0.2293 | val_loss(log-MSE)=0.2901 | val_MAE(hours)=66.79\n",
            "\n",
            "========== Epoch 10/20 ==========\n",
            "  [Epoch 10 | Step 0/1162] AvgTrainLoss=0.2089\n",
            "  [Epoch 10 | Step 100/1162] AvgTrainLoss=0.2152\n",
            "  [Epoch 10 | Step 200/1162] AvgTrainLoss=0.2173\n",
            "  [Epoch 10 | Step 300/1162] AvgTrainLoss=0.2192\n",
            "  [Epoch 10 | Step 400/1162] AvgTrainLoss=0.2205\n",
            "  [Epoch 10 | Step 500/1162] AvgTrainLoss=0.2206\n",
            "  [Epoch 10 | Step 600/1162] AvgTrainLoss=0.2211\n",
            "  [Epoch 10 | Step 700/1162] AvgTrainLoss=0.2218\n",
            "  [Epoch 10 | Step 800/1162] AvgTrainLoss=0.2221\n",
            "  [Epoch 10 | Step 900/1162] AvgTrainLoss=0.2228\n",
            "  [Epoch 10 | Step 1000/1162] AvgTrainLoss=0.2233\n",
            "  [Epoch 10 | Step 1100/1162] AvgTrainLoss=0.2239\n",
            "[Epoch 010] train_loss(log-MSE)=0.2243 | val_loss(log-MSE)=0.2561 | val_MAE(hours)=60.84\n",
            "  → Best model updated, saved to los_multibranch_best.pt\n",
            "\n",
            "========== Epoch 11/20 ==========\n",
            "  [Epoch 11 | Step 0/1162] AvgTrainLoss=0.2229\n",
            "  [Epoch 11 | Step 100/1162] AvgTrainLoss=0.2104\n",
            "  [Epoch 11 | Step 200/1162] AvgTrainLoss=0.2133\n",
            "  [Epoch 11 | Step 300/1162] AvgTrainLoss=0.2140\n",
            "  [Epoch 11 | Step 400/1162] AvgTrainLoss=0.2154\n",
            "  [Epoch 11 | Step 500/1162] AvgTrainLoss=0.2158\n",
            "  [Epoch 11 | Step 600/1162] AvgTrainLoss=0.2160\n",
            "  [Epoch 11 | Step 700/1162] AvgTrainLoss=0.2170\n",
            "  [Epoch 11 | Step 800/1162] AvgTrainLoss=0.2178\n",
            "  [Epoch 11 | Step 900/1162] AvgTrainLoss=0.2183\n",
            "  [Epoch 11 | Step 1000/1162] AvgTrainLoss=0.2189\n",
            "  [Epoch 11 | Step 1100/1162] AvgTrainLoss=0.2191\n",
            "[Epoch 011] train_loss(log-MSE)=0.2193 | val_loss(log-MSE)=0.2585 | val_MAE(hours)=59.34\n",
            "\n",
            "========== Epoch 12/20 ==========\n",
            "  [Epoch 12 | Step 0/1162] AvgTrainLoss=0.2107\n",
            "  [Epoch 12 | Step 100/1162] AvgTrainLoss=0.2082\n",
            "  [Epoch 12 | Step 200/1162] AvgTrainLoss=0.2082\n",
            "  [Epoch 12 | Step 300/1162] AvgTrainLoss=0.2101\n",
            "  [Epoch 12 | Step 400/1162] AvgTrainLoss=0.2120\n",
            "  [Epoch 12 | Step 500/1162] AvgTrainLoss=0.2120\n",
            "  [Epoch 12 | Step 600/1162] AvgTrainLoss=0.2128\n",
            "  [Epoch 12 | Step 700/1162] AvgTrainLoss=0.2130\n",
            "  [Epoch 12 | Step 800/1162] AvgTrainLoss=0.2131\n",
            "  [Epoch 12 | Step 900/1162] AvgTrainLoss=0.2143\n",
            "  [Epoch 12 | Step 1000/1162] AvgTrainLoss=0.2149\n",
            "  [Epoch 12 | Step 1100/1162] AvgTrainLoss=0.2153\n",
            "[Epoch 012] train_loss(log-MSE)=0.2154 | val_loss(log-MSE)=0.2616 | val_MAE(hours)=61.62\n",
            "\n",
            "========== Epoch 13/20 ==========\n",
            "  [Epoch 13 | Step 0/1162] AvgTrainLoss=0.2036\n",
            "  [Epoch 13 | Step 100/1162] AvgTrainLoss=0.2022\n",
            "  [Epoch 13 | Step 200/1162] AvgTrainLoss=0.2033\n",
            "  [Epoch 13 | Step 300/1162] AvgTrainLoss=0.2053\n",
            "  [Epoch 13 | Step 400/1162] AvgTrainLoss=0.2066\n",
            "  [Epoch 13 | Step 500/1162] AvgTrainLoss=0.2075\n",
            "  [Epoch 13 | Step 600/1162] AvgTrainLoss=0.2083\n",
            "  [Epoch 13 | Step 700/1162] AvgTrainLoss=0.2090\n",
            "  [Epoch 13 | Step 800/1162] AvgTrainLoss=0.2096\n",
            "  [Epoch 13 | Step 900/1162] AvgTrainLoss=0.2097\n",
            "  [Epoch 13 | Step 1000/1162] AvgTrainLoss=0.2104\n",
            "  [Epoch 13 | Step 1100/1162] AvgTrainLoss=0.2109\n",
            "[Epoch 013] train_loss(log-MSE)=0.2113 | val_loss(log-MSE)=0.3046 | val_MAE(hours)=70.93\n",
            "\n",
            "========== Epoch 14/20 ==========\n",
            "  [Epoch 14 | Step 0/1162] AvgTrainLoss=0.1928\n",
            "  [Epoch 14 | Step 100/1162] AvgTrainLoss=0.1959\n",
            "  [Epoch 14 | Step 200/1162] AvgTrainLoss=0.1993\n",
            "  [Epoch 14 | Step 300/1162] AvgTrainLoss=0.1997\n",
            "  [Epoch 14 | Step 400/1162] AvgTrainLoss=0.2005\n",
            "  [Epoch 14 | Step 500/1162] AvgTrainLoss=0.2014\n",
            "  [Epoch 14 | Step 600/1162] AvgTrainLoss=0.2033\n",
            "  [Epoch 14 | Step 700/1162] AvgTrainLoss=0.2040\n",
            "  [Epoch 14 | Step 800/1162] AvgTrainLoss=0.2043\n",
            "  [Epoch 14 | Step 900/1162] AvgTrainLoss=0.2050\n",
            "  [Epoch 14 | Step 1000/1162] AvgTrainLoss=0.2055\n",
            "  [Epoch 14 | Step 1100/1162] AvgTrainLoss=0.2060\n",
            "[Epoch 014] train_loss(log-MSE)=0.2063 | val_loss(log-MSE)=0.2655 | val_MAE(hours)=60.31\n",
            "\n",
            "========== Epoch 15/20 ==========\n",
            "  [Epoch 15 | Step 0/1162] AvgTrainLoss=0.1847\n",
            "  [Epoch 15 | Step 100/1162] AvgTrainLoss=0.1951\n",
            "  [Epoch 15 | Step 200/1162] AvgTrainLoss=0.1962\n",
            "  [Epoch 15 | Step 300/1162] AvgTrainLoss=0.1970\n",
            "  [Epoch 15 | Step 400/1162] AvgTrainLoss=0.1984\n",
            "  [Epoch 15 | Step 500/1162] AvgTrainLoss=0.1995\n",
            "  [Epoch 15 | Step 600/1162] AvgTrainLoss=0.1999\n",
            "  [Epoch 15 | Step 700/1162] AvgTrainLoss=0.2004\n",
            "  [Epoch 15 | Step 800/1162] AvgTrainLoss=0.2009\n",
            "  [Epoch 15 | Step 900/1162] AvgTrainLoss=0.2018\n",
            "  [Epoch 15 | Step 1000/1162] AvgTrainLoss=0.2023\n",
            "  [Epoch 15 | Step 1100/1162] AvgTrainLoss=0.2030\n",
            "[Epoch 015] train_loss(log-MSE)=0.2034 | val_loss(log-MSE)=0.3012 | val_MAE(hours)=66.87\n",
            "\n",
            "========== Epoch 16/20 ==========\n",
            "  [Epoch 16 | Step 0/1162] AvgTrainLoss=0.1654\n",
            "  [Epoch 16 | Step 100/1162] AvgTrainLoss=0.1956\n",
            "  [Epoch 16 | Step 200/1162] AvgTrainLoss=0.1950\n",
            "  [Epoch 16 | Step 300/1162] AvgTrainLoss=0.1943\n",
            "  [Epoch 16 | Step 400/1162] AvgTrainLoss=0.1956\n",
            "  [Epoch 16 | Step 500/1162] AvgTrainLoss=0.1963\n",
            "  [Epoch 16 | Step 600/1162] AvgTrainLoss=0.1967\n",
            "  [Epoch 16 | Step 700/1162] AvgTrainLoss=0.1977\n",
            "  [Epoch 16 | Step 800/1162] AvgTrainLoss=0.1987\n",
            "  [Epoch 16 | Step 900/1162] AvgTrainLoss=0.1989\n",
            "  [Epoch 16 | Step 1000/1162] AvgTrainLoss=0.1995\n",
            "  [Epoch 16 | Step 1100/1162] AvgTrainLoss=0.2002\n",
            "[Epoch 016] train_loss(log-MSE)=0.2006 | val_loss(log-MSE)=0.2809 | val_MAE(hours)=64.82\n",
            "\n",
            "========== Epoch 17/20 ==========\n",
            "  [Epoch 17 | Step 0/1162] AvgTrainLoss=0.2219\n",
            "  [Epoch 17 | Step 100/1162] AvgTrainLoss=0.1889\n",
            "  [Epoch 17 | Step 200/1162] AvgTrainLoss=0.1893\n",
            "  [Epoch 17 | Step 300/1162] AvgTrainLoss=0.1910\n",
            "  [Epoch 17 | Step 400/1162] AvgTrainLoss=0.1920\n",
            "  [Epoch 17 | Step 500/1162] AvgTrainLoss=0.1922\n",
            "  [Epoch 17 | Step 600/1162] AvgTrainLoss=0.1929\n",
            "  [Epoch 17 | Step 700/1162] AvgTrainLoss=0.1937\n",
            "  [Epoch 17 | Step 800/1162] AvgTrainLoss=0.1946\n",
            "  [Epoch 17 | Step 900/1162] AvgTrainLoss=0.1953\n",
            "  [Epoch 17 | Step 1000/1162] AvgTrainLoss=0.1962\n",
            "  [Epoch 17 | Step 1100/1162] AvgTrainLoss=0.1968\n",
            "[Epoch 017] train_loss(log-MSE)=0.1969 | val_loss(log-MSE)=0.2761 | val_MAE(hours)=63.99\n",
            "\n",
            "========== Epoch 18/20 ==========\n",
            "  [Epoch 18 | Step 0/1162] AvgTrainLoss=0.1713\n",
            "  [Epoch 18 | Step 100/1162] AvgTrainLoss=0.1850\n",
            "  [Epoch 18 | Step 200/1162] AvgTrainLoss=0.1869\n",
            "  [Epoch 18 | Step 300/1162] AvgTrainLoss=0.1883\n",
            "  [Epoch 18 | Step 400/1162] AvgTrainLoss=0.1893\n",
            "  [Epoch 18 | Step 500/1162] AvgTrainLoss=0.1896\n",
            "  [Epoch 18 | Step 600/1162] AvgTrainLoss=0.1903\n",
            "  [Epoch 18 | Step 700/1162] AvgTrainLoss=0.1914\n",
            "  [Epoch 18 | Step 800/1162] AvgTrainLoss=0.1922\n",
            "  [Epoch 18 | Step 900/1162] AvgTrainLoss=0.1925\n",
            "  [Epoch 18 | Step 1000/1162] AvgTrainLoss=0.1932\n",
            "  [Epoch 18 | Step 1100/1162] AvgTrainLoss=0.1936\n",
            "[Epoch 018] train_loss(log-MSE)=0.1940 | val_loss(log-MSE)=0.2507 | val_MAE(hours)=58.75\n",
            "  → Best model updated, saved to los_multibranch_best.pt\n",
            "\n",
            "========== Epoch 19/20 ==========\n",
            "  [Epoch 19 | Step 0/1162] AvgTrainLoss=0.1691\n",
            "  [Epoch 19 | Step 100/1162] AvgTrainLoss=0.1814\n",
            "  [Epoch 19 | Step 200/1162] AvgTrainLoss=0.1835\n",
            "  [Epoch 19 | Step 300/1162] AvgTrainLoss=0.1836\n",
            "  [Epoch 19 | Step 400/1162] AvgTrainLoss=0.1853\n",
            "  [Epoch 19 | Step 500/1162] AvgTrainLoss=0.1864\n",
            "  [Epoch 19 | Step 600/1162] AvgTrainLoss=0.1873\n",
            "  [Epoch 19 | Step 700/1162] AvgTrainLoss=0.1880\n",
            "  [Epoch 19 | Step 800/1162] AvgTrainLoss=0.1888\n",
            "  [Epoch 19 | Step 900/1162] AvgTrainLoss=0.1894\n",
            "  [Epoch 19 | Step 1000/1162] AvgTrainLoss=0.1899\n",
            "  [Epoch 19 | Step 1100/1162] AvgTrainLoss=0.1908\n",
            "[Epoch 019] train_loss(log-MSE)=0.1911 | val_loss(log-MSE)=0.2531 | val_MAE(hours)=59.00\n",
            "\n",
            "========== Epoch 20/20 ==========\n",
            "  [Epoch 20 | Step 0/1162] AvgTrainLoss=0.2081\n",
            "  [Epoch 20 | Step 100/1162] AvgTrainLoss=0.1786\n",
            "  [Epoch 20 | Step 200/1162] AvgTrainLoss=0.1816\n",
            "  [Epoch 20 | Step 300/1162] AvgTrainLoss=0.1834\n",
            "  [Epoch 20 | Step 400/1162] AvgTrainLoss=0.1842\n",
            "  [Epoch 20 | Step 500/1162] AvgTrainLoss=0.1857\n",
            "  [Epoch 20 | Step 600/1162] AvgTrainLoss=0.1858\n",
            "  [Epoch 20 | Step 700/1162] AvgTrainLoss=0.1858\n",
            "  [Epoch 20 | Step 800/1162] AvgTrainLoss=0.1867\n",
            "  [Epoch 20 | Step 900/1162] AvgTrainLoss=0.1873\n",
            "  [Epoch 20 | Step 1000/1162] AvgTrainLoss=0.1876\n",
            "  [Epoch 20 | Step 1100/1162] AvgTrainLoss=0.1883\n",
            "[Epoch 020] train_loss(log-MSE)=0.1886 | val_loss(log-MSE)=0.2720 | val_MAE(hours)=62.86\n",
            "Training finished. Best val_loss: 0.2506806408636838\n"
          ]
        }
      ],
      "source": [
        "# 1. Model / Config Creation\n",
        "\n",
        "cfg = ModelConfig(\n",
        "    num_genders=len(gender_stoi),\n",
        "    num_races=len(race_stoi),\n",
        "    num_services=len(service_stoi),\n",
        "    num_drg_codes=len(drg_code_stoi),\n",
        "    diag_vocab_size=len(diag_stoi),\n",
        "    proc_vocab_size=len(proc_all_stoi),\n",
        "    med_vocab_size=len(med_stoi),\n",
        "    order_vocab_size=len(order_stoi),\n",
        ")\n",
        "\n",
        "model = MultiModalLOSModel(cfg)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "criterion = nn.MSELoss()  # MSE based on log(1+LOS)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "\n",
        "NUM_EPOCHS = 20 \n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "best_model_path = \"los_multibranch_best.pt\"\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "\n",
        "# 2. Training Loop\n",
        "for epoch in range(1, NUM_EPOCHS + 1):\n",
        "    # ---------- Train ----------\n",
        "    model.train()\n",
        "    train_loss_sum = 0.0\n",
        "    train_count = 0\n",
        "\n",
        "    print(f\"\\n========== Epoch {epoch}/{NUM_EPOCHS} ==========\")\n",
        "\n",
        "    for batch_idx, batch in enumerate(train_loader):\n",
        "        # Move tensors to device\n",
        "        batch = {k: (v.to(device) if torch.is_tensor(v) else v)\n",
        "                 for k, v in batch.items()}\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred = model(\n",
        "            age=batch[\"age\"],\n",
        "            gender_idx=batch[\"gender_id\"],\n",
        "            race_idx=batch[\"race_id\"],\n",
        "            service_idx=batch[\"service_id\"],\n",
        "            drg_code_idx=batch[\"drg_code_id\"],\n",
        "            drg_severity=batch[\"drg_severity\"],\n",
        "            drg_mortality=batch[\"drg_mortality\"],\n",
        "            diag_codes=batch[\"diag_codes\"],\n",
        "            diag_offsets=batch[\"diag_offsets\"],\n",
        "            proc_codes=batch[\"proc_codes\"],\n",
        "            proc_offsets=batch[\"proc_offsets\"],\n",
        "            med_codes=batch[\"med_codes\"],\n",
        "            med_offsets=batch[\"med_offsets\"],\n",
        "            order_codes=batch[\"order_codes\"],\n",
        "            order_offsets=batch[\"order_offsets\"],\n",
        "        )\n",
        "\n",
        "        y_true = batch[\"target\"]  # log(1+LOS)\n",
        "        loss = criterion(y_pred, y_true)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        bs = y_true.size(0)\n",
        "        train_loss_sum += loss.item() * bs\n",
        "        train_count += bs\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            avg_loss = train_loss_sum / train_count\n",
        "            print(f\"  [Epoch {epoch} | Step {batch_idx}/{len(train_loader)}] \"\n",
        "                  f\"AvgTrainLoss={avg_loss:.4f}\")\n",
        "\n",
        "    train_loss = train_loss_sum / train_count\n",
        "\n",
        "    # ---------- Validation ----------\n",
        "    model.eval()\n",
        "    val_loss_sum = 0.0\n",
        "    val_count = 0\n",
        "    val_mae_hours_sum = 0.0  # MAE based on actual LOS (hours)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            batch = {k: (v.to(device) if torch.is_tensor(v) else v)\n",
        "                     for k, v in batch.items()}\n",
        "\n",
        "            y_pred = model(\n",
        "                age=batch[\"age\"],\n",
        "                gender_idx=batch[\"gender_id\"],\n",
        "                race_idx=batch[\"race_id\"],\n",
        "                service_idx=batch[\"service_id\"],\n",
        "                drg_code_idx=batch[\"drg_code_id\"],\n",
        "                drg_severity=batch[\"drg_severity\"],\n",
        "                drg_mortality=batch[\"drg_mortality\"],\n",
        "                diag_codes=batch[\"diag_codes\"],\n",
        "                diag_offsets=batch[\"diag_offsets\"],\n",
        "                proc_codes=batch[\"proc_codes\"],\n",
        "                proc_offsets=batch[\"proc_offsets\"],\n",
        "                med_codes=batch[\"med_codes\"],\n",
        "                med_offsets=batch[\"med_offsets\"],\n",
        "                order_codes=batch[\"order_codes\"],\n",
        "                order_offsets=batch[\"order_offsets\"],\n",
        "            )\n",
        "\n",
        "            y_true = batch[\"target\"]\n",
        "\n",
        "            loss = criterion(y_pred, y_true)\n",
        "\n",
        "            bs = y_true.size(0)\n",
        "            val_loss_sum += loss.item() * bs\n",
        "            val_count += bs\n",
        "\n",
        "            # Convert log(1+LOS) -> actual LOS (hours) and calculate MAE\n",
        "            y_true_hours = torch.expm1(y_true)\n",
        "            y_pred_hours = torch.expm1(y_pred)\n",
        "\n",
        "            mae_hours = torch.abs(y_pred_hours - y_true_hours).sum().item()\n",
        "            val_mae_hours_sum += mae_hours\n",
        "\n",
        "    val_loss = val_loss_sum / val_count\n",
        "    val_mae_hours = val_mae_hours_sum / val_count\n",
        "\n",
        "    print(f\"[Epoch {epoch:03d}] \"\n",
        "          f\"train_loss(log-MSE)={train_loss:.4f} | \"\n",
        "          f\"val_loss(log-MSE)={val_loss:.4f} | \"\n",
        "          f\"val_MAE(hours)={val_mae_hours:.2f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), best_model_path)\n",
        "        print(f\"  → Best model updated, saved to {best_model_path}\")\n",
        "\n",
        "print(\"Training finished. Best val_loss:\", best_val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vDQN05751XD",
        "outputId": "3c8e4c9d-f9bf-4a90-c1ed-6a652879ba97"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Test set MAE =====\n",
            "Test MAE (hours): 58.48\n",
            "Test MAE (days) : 2.44\n"
          ]
        }
      ],
      "source": [
        "# 3. Calculate MAE (hours) on Test set\n",
        "\n",
        "best_model = MultiModalLOSModel(cfg).to(device)\n",
        "best_model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "best_model.eval()\n",
        "\n",
        "test_abs_error_sum = 0.0\n",
        "test_count = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: (v.to(device) if torch.is_tensor(v) else v)\n",
        "                 for k, v in batch.items()}\n",
        "\n",
        "        y_pred = best_model(\n",
        "            age=batch[\"age\"],\n",
        "            gender_idx=batch[\"gender_id\"],\n",
        "            race_idx=batch[\"race_id\"],\n",
        "            service_idx=batch[\"service_id\"],\n",
        "            drg_code_idx=batch[\"drg_code_id\"],\n",
        "            drg_severity=batch[\"drg_severity\"],\n",
        "            drg_mortality=batch[\"drg_mortality\"],\n",
        "            diag_codes=batch[\"diag_codes\"],\n",
        "            diag_offsets=batch[\"diag_offsets\"],\n",
        "            proc_codes=batch[\"proc_codes\"],\n",
        "            proc_offsets=batch[\"proc_offsets\"],\n",
        "            med_codes=batch[\"med_codes\"],\n",
        "            med_offsets=batch[\"med_offsets\"],\n",
        "            order_codes=batch[\"order_codes\"],\n",
        "            order_offsets=batch[\"order_offsets\"],\n",
        "        )\n",
        "\n",
        "        y_true = batch[\"target\"]  # log(1+LOS)\n",
        "\n",
        "        # Convert log(1+LOS) → actual hours\n",
        "        y_true_hours = torch.expm1(y_true)\n",
        "        y_pred_hours = torch.expm1(y_pred)\n",
        "\n",
        "        abs_err = torch.abs(y_pred_hours - y_true_hours)\n",
        "        test_abs_error_sum += abs_err.sum().item()\n",
        "        test_count += y_true.size(0)\n",
        "\n",
        "test_mae_hours = test_abs_error_sum / test_count\n",
        "\n",
        "print(f\"\\n===== Test set MAE =====\")\n",
        "print(f\"Test MAE (hours): {test_mae_hours:.2f}\")\n",
        "print(f\"Test MAE (days) : {test_mae_hours / 24:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "mpmdUCi5FTdd",
        "outputId": "f548136f-dd19-4566-93df-38adfc1e0140"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "all_abs_errors = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in test_loader:\n",
        "        batch = {k: (v.to(device) if torch.is_tensor(v) else v)\n",
        "                 for k, v in batch.items()}\n",
        "\n",
        "        y_pred = best_model(\n",
        "            age=batch[\"age\"],\n",
        "            gender_idx=batch[\"gender_id\"],\n",
        "            race_idx=batch[\"race_id\"],\n",
        "            service_idx=batch[\"service_id\"],\n",
        "            drg_code_idx=batch[\"drg_code_id\"],\n",
        "            drg_severity=batch[\"drg_severity\"],\n",
        "            drg_mortality=batch[\"drg_mortality\"],\n",
        "            diag_codes=batch[\"diag_codes\"],\n",
        "            diag_offsets=batch[\"diag_offsets\"],\n",
        "            proc_codes=batch[\"proc_codes\"],\n",
        "            proc_offsets=batch[\"proc_offsets\"],\n",
        "            med_codes=batch[\"med_codes\"],\n",
        "            med_offsets=batch[\"med_offsets\"],\n",
        "            order_codes=batch[\"order_codes\"],\n",
        "            order_offsets=batch[\"order_offsets\"],\n",
        "        )\n",
        "\n",
        "        y_true = batch[\"target\"]  # log(1+LOS)\n",
        "\n",
        "        y_true_hours = torch.expm1(y_true)\n",
        "        y_pred_hours = torch.expm1(y_pred)\n",
        "\n",
        "        abs_err = torch.abs(y_pred_hours - y_true_hours)\n",
        "\n",
        "        all_abs_errors.extend(abs_err.cpu().numpy())\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(\n",
        "    all_abs_errors,\n",
        "    bins=100,              # Number of bins\n",
        "    range=(0, 500),        # X-axis range (0~500 hours)\n",
        "    edgecolor='black',\n",
        "    alpha=0.75\n",
        ")\n",
        "\n",
        "plt.title(\"Test Set Prediction Error Distribution (Hours)\")\n",
        "plt.xlabel(\"Absolute Error (hours)\")\n",
        "plt.ylabel(\"Frequency (count)\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NPZsDxsmvgCy",
        "outputId": "bbf211db-0240-4276-b43b-366f9344a55a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "====== Sample Predictions (True vs Predicted LOS) ======\n",
            "\n",
            "True LOS: 44.72 hours | Predicted: 68.90 hours | Error: 24.19\n",
            "True LOS: 55.63 hours | Predicted: 119.59 hours | Error: 63.96\n",
            "True LOS: 90.42 hours | Predicted: 158.60 hours | Error: 68.18\n",
            "True LOS: 24.08 hours | Predicted: 75.49 hours | Error: 51.41\n",
            "True LOS: 106.88 hours | Predicted: 46.30 hours | Error: 60.58\n",
            "True LOS: 247.07 hours | Predicted: 109.92 hours | Error: 137.15\n",
            "True LOS: 261.08 hours | Predicted: 106.67 hours | Error: 154.41\n",
            "True LOS: 35.00 hours | Predicted: 44.37 hours | Error: 9.37\n",
            "True LOS: 63.13 hours | Predicted: 69.42 hours | Error: 6.28\n",
            "True LOS: 102.85 hours | Predicted: 151.06 hours | Error: 48.21\n",
            "True LOS: 80.08 hours | Predicted: 65.83 hours | Error: 14.25\n",
            "True LOS: 44.72 hours | Predicted: 76.55 hours | Error: 31.84\n",
            "True LOS: 255.52 hours | Predicted: 95.18 hours | Error: 160.34\n",
            "True LOS: 45.53 hours | Predicted: 43.22 hours | Error: 2.31\n",
            "True LOS: 840.62 hours | Predicted: 529.14 hours | Error: 311.48\n",
            "True LOS: 132.08 hours | Predicted: 81.53 hours | Error: 50.55\n",
            "True LOS: 301.33 hours | Predicted: 138.02 hours | Error: 163.32\n",
            "True LOS: 265.32 hours | Predicted: 68.48 hours | Error: 196.83\n",
            "True LOS: 37.37 hours | Predicted: 61.82 hours | Error: 24.46\n",
            "True LOS: 373.82 hours | Predicted: 358.02 hours | Error: 15.80\n"
          ]
        }
      ],
      "source": [
        "# Output actual vs predicted values\n",
        "\n",
        "model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "print(\"\\n====== Sample Predictions (True vs Predicted LOS) ======\\n\")\n",
        "\n",
        "num_show = 20 \n",
        "shown = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in val_loader:\n",
        "        batch = {k: (v.to(device) if torch.is_tensor(v) else v)\n",
        "                 for k, v in batch.items()}\n",
        "\n",
        "        y_pred = model(\n",
        "            age=batch[\"age\"],\n",
        "            gender_idx=batch[\"gender_id\"],\n",
        "            race_idx=batch[\"race_id\"],\n",
        "            service_idx=batch[\"service_id\"],\n",
        "            drg_code_idx=batch[\"drg_code_id\"],\n",
        "            drg_severity=batch[\"drg_severity\"],\n",
        "            drg_mortality=batch[\"drg_mortality\"],\n",
        "            diag_codes=batch[\"diag_codes\"],\n",
        "            diag_offsets=batch[\"diag_offsets\"],\n",
        "            proc_codes=batch[\"proc_codes\"],\n",
        "            proc_offsets=batch[\"proc_offsets\"],\n",
        "            med_codes=batch[\"med_codes\"],\n",
        "            med_offsets=batch[\"med_offsets\"],\n",
        "            order_codes=batch[\"order_codes\"],\n",
        "            order_offsets=batch[\"order_offsets\"],\n",
        "        )\n",
        "\n",
        "        y_true = batch[\"target\"]\n",
        "\n",
        "        # Convert log-scale → hours\n",
        "        y_true_hours = torch.expm1(y_true).cpu().numpy()\n",
        "        y_pred_hours = torch.expm1(y_pred).cpu().numpy()\n",
        "\n",
        "        for t, p in zip(y_true_hours, y_pred_hours):\n",
        "            print(f\"True LOS: {t:.2f} hours | Predicted: {p:.2f} hours | Error: {abs(t - p):.2f}\")\n",
        "            shown += 1\n",
        "            if shown >= num_show:\n",
        "                break\n",
        "\n",
        "        if shown >= num_show:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndxFSOmYR6Z7"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
